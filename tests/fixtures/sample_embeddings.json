[
  {
    "id": "emb1",
    "paper_id": "paper1",
    "chunk_index": 0,
    "chunk_text": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism.",
    "embedding": [0.023, -0.145, 0.089, 0.234, -0.067, 0.156, 0.078, -0.123, 0.201, 0.045]
  },
  {
    "id": "emb2",
    "paper_id": "paper1",
    "chunk_index": 1,
    "chunk_text": "We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.",
    "embedding": [0.134, -0.078, 0.023, 0.189, -0.145, 0.234, 0.067, -0.201, 0.156, 0.089]
  },
  {
    "id": "emb3",
    "paper_id": "paper2",
    "chunk_index": 0,
    "chunk_text": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations.",
    "embedding": [0.089, -0.234, 0.145, 0.067, -0.201, 0.123, 0.156, -0.045, 0.078, 0.189]
  },
  {
    "id": "emb4",
    "paper_id": "paper3",
    "chunk_index": 0,
    "chunk_text": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets.",
    "embedding": [0.156, -0.089, 0.234, 0.023, -0.145, 0.201, 0.078, -0.067, 0.189, 0.123]
  }
]
