{
  "nodes": [
    {
      "id": "paper1",
      "title": "Attention Is All You Need",
      "authors": ["Vaswani, A.", "Shazeer, N."],
      "year": 2017,
      "pdf_path": "paper1.pdf",
      "markdown_path": "paper1_markdown.md",
      "obsidian_path": "paper1.md",
      "citation_count": 50000
    },
    {
      "id": "paper2",
      "title": "BERT: Pre-training of Deep Bidirectional Transformers",
      "authors": ["Devlin, J.", "Chang, M-W."],
      "year": 2019,
      "pdf_path": "paper2.pdf",
      "markdown_path": "paper2_markdown.md",
      "obsidian_path": "paper2.md",
      "citation_count": 30000
    },
    {
      "id": "paper3",
      "title": "GPT-3: Language Models are Few-Shot Learners",
      "authors": ["Brown, T.", "Mann, B."],
      "year": 2020,
      "pdf_path": "paper3.pdf",
      "markdown_path": "paper3_markdown.md",
      "obsidian_path": "paper3.md",
      "citation_count": 20000
    }
  ],
  "edges": [
    {
      "source": "paper2",
      "target": "paper1",
      "type": "citation",
      "context": "Building on the transformer architecture..."
    },
    {
      "source": "paper3",
      "target": "paper1",
      "type": "citation",
      "context": "Using self-attention mechanisms..."
    },
    {
      "source": "paper3",
      "target": "paper2",
      "type": "citation",
      "context": "Extending pre-training approaches..."
    }
  ]
}
