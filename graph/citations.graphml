{
  "directed": true,
  "multigraph": false,
  "graph": {},
  "nodes": [
    {
      "metadata": {
        "text": "Distillation of encoder-decoder transformers for sequence labellingMarco Farina  Duccio Pappadopulo Anant GuptaLeslie Huang Ozan  I rsoy Thamar Solorio Bloomberg Department of Computer Science, University of Houston{mfarina19, dpappadopulo, aqupta968, lhuang328, oirsoy}@bloomberg.nettsolorio@uh.edu#### AbstractDriven by encouraging results on a wide range of tasks, the field of NLP is experiencing an accelerated race to develop bigger language models. This race for bigger models has also underscored the need to continue the pursuit of practical distillation approaches that can leverage the knowledge acquired by these big models in a compute-efficient manner. Having this goal in mind, we build on recent work to propose a hallucination-free framework for sequence tagging that is especially suited for distillation. We show empirical results of new state-of-the-art performa",
        "authors": [
          "Farina, Marco",
          "Pappadopulo, Duccio",
          "Gupta, Anant",
          "Huang, Leslie",
          "\u0130rsoy, Ozan",
          "Solorio, Thamar"
        ],
        "affiliations": [
          "Bloomberg",
          "Department of Computer Science, University of Houston"
        ],
        "title": "Distillation of encoder-decoder transformers for sequence labelling",
        "abstract": "Driven by encouraging results on a wide range of tasks, the field of NLP is experiencing an accelerated race to develop bigger language models. This race for bigger models has also underscored the need to continue the pursuit of practical distillation approaches that can leverage the knowledge acquired by these big models in a compute-efficient manner. Having this goal in mind, we build on recent work to propose a hallucination-free framework for sequence tagging that is especially suited for distillation. We show empirical results of new state-of-the-art performance across multiple sequence labelling datasets and validate the usefulness of this framework for distilling a large model in a few-shot learning scenario.",
        "year": 2023,
        "journal": "ArXiv",
        "venue": "Findings",
        "citation_count": 3,
        "doi": "10.48550/arXiv.2302.05454",
        "backup_id": null,
        "url": "https://www.semanticscholar.org/paper/0704a96e1c57c12031f1c3ca492a91dbed1f61ce",
        "volume": null,
        "issue": null,
        "pages": null,
        "is_document_citation": true,
        "formatted": null,
        "fields_of_study": [
          "Computer Science"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "doi:10.48550/arXiv.2302.05454"
    },
    {
      "metadata": {
        "text": "SLS corpora. https://groups.csail.mit.edu/sls/downloads/. Accessed: 2022-09-09.",
        "authors": [
          "D. Paul",
          "J. Baker"
        ],
        "affiliations": null,
        "title": "SLS corpora",
        "abstract": "The DARPA Spoken Language System (SLS) community has long taken a leadership position in designing, implementing, and globally distributing significant speech corpora widely used for advancing speech recognition research. The Wall Street Journal (WSJ) CSR Corpus described here is the newest addition to this valuable set of resources. In contrast to previous corpora, the WSJ corpus will provide DARPA its first general-purpose English, large vocabulary, natural language, high perplexity, corpus containing significant quantities of both speech data (400 hrs.) and text data (47M words), thereby providing a means to integrate speech recognition and natural language processing in application domains with high potential practical value. This paper presents the motivating goals, acoustic data design, text processing steps, lexicons, and testing paradigms incorporated into the multi-faceted WSJ CSR Corpus.",
        "year": 1992,
        "journal": null,
        "venue": "Human Language Technology - The Baltic Perspectiv",
        "citation_count": 1490,
        "doi": "10.21437/ICSLP.1992-277",
        "backup_id": null,
        "url": "https://groups.csail.mit.edu/sls/downloads/",
        "volume": null,
        "issue": null,
        "pages": null,
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Computer Science"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "doi:10.21437/ICSLP.1992-277"
    },
    {
      "metadata": {
        "text": "Mikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, Sam Shleifer, Xi Victoria Lin, Jingfei Du, Srinivasan Iyer, Ramakanth Pasunuru, Giri Anantharaman, Xian Li, Shuohui Chen, Halil Akin, Mandeep Baines, Louis Martin, Xing Zhou, Punit Singh Koura, Brian O'Horo, Jeff Wang, Luke Zettlemoyer, Mona Diab, Zornitsa Kozareva, and Ves Stoyanov. 2021. Efficient large scale language modeling with mixtures of experts.",
        "authors": [
          "Artetxe, M",
          " Bhosale, S",
          " Goyal, N",
          " Mihaylov, T",
          " Ott, M",
          " Shleifer, S",
          " Lin, X",
          " Du, J",
          " Iyer, S",
          " Pasunuru, R",
          " Anantharaman, G",
          " Li, X",
          " Chen, S",
          " Akin, H",
          " Baines, M",
          " Martin, L",
          " Zhou, X",
          " Koura, P",
          " O'Horo, B",
          " Wang, J",
          " Zettlemoyer, L",
          " Diab, M",
          " Kozareva, Z",
          " Stoyanov, V"
        ],
        "affiliations": null,
        "title": "Efficient large scale language modeling with mixtures of experts",
        "abstract": "Mixture of Experts layers (MoEs) enable efficient scaling of language models through conditional computation. This paper presents a detailed empirical study of how autoregressive MoE language models scale in comparison with dense models in a wide range of settings: in- and out-of-domain language modeling, zero- and few-shot priming, and full-shot fine-tuning. With the exception of fine-tuning, we find MoEs to be substantially more compute efficient. At more modest training budgets, MoEs can match the performance of dense models using $\\sim$4 times less compute. This gap narrows at scale, but our largest MoE model (1.1T parameters) consistently outperforms a compute-equivalent dense model (6.7B parameters). Overall, this performance gap varies greatly across tasks and domains, suggesting that MoE and dense models generalize differently in ways that are worthy of future study. We make our code and models publicly available for research use.",
        "year": 2021,
        "journal": "ArXiv",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citation_count": 188,
        "doi": "10.18653/v1/2022.emnlp-main.804",
        "backup_id": null,
        "url": "https://www.semanticscholar.org/paper/fb01415a0decfa3f3d6339930e95028ae1ff4170",
        "volume": null,
        "issue": null,
        "pages": null,
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Computer Science"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "doi:10.18653/v1/2022.emnlp-main.804"
    },
    {
      "metadata": {
        "text": "Ben Athiwaratkun, Cicero Nogueira dos Santos, Jason Krone, and Bing Xiang. 2020. Augmented natural language for generative sequence labeling. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 375-385, Online. Association for Computational Linguistics.",
        "authors": [
          "Ben Athiwaratkun",
          "Cicero Nogueira dos Santos",
          "Jason Krone",
          "Bing Xiang"
        ],
        "affiliations": null,
        "title": "Augmented natural language for generative sequence labeling",
        "abstract": "We propose a generative framework for joint sequence labeling and sentence-level classification. Our model performs multiple sequence labeling tasks at once using a single, shared natural language output space. Unlike prior discriminative methods, our model naturally incorporates label semantics and shares knowledge across tasks. Our framework is general purpose, performing well on few-shot, low-resource, and high-resource tasks. We demonstrate these advantages on popular named entity recognition, slot labeling, and intent classification benchmarks. We set a new state-of-the-art for few-shot slot labeling, improving substantially upon the previous 5-shot ($75.0\\% \\rightarrow 90.9\\%$) and 1-shot ($70.4\\% \\rightarrow 81.0\\%$) state-of-the-art results. Furthermore, our model generates large improvements ($46.27\\% \\rightarrow 63.83\\%$) in low-resource slot labeling over a BERT baseline by incorporating label semantics. We also maintain competitive results on high-resource tasks, performing within two points of the state-of-the-art on all tasks and setting a new state-of-the-art on the SNIPS dataset.",
        "year": 2020,
        "journal": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citation_count": 60,
        "doi": "10.18653/v1/2020.emnlp-main.27",
        "backup_id": null,
        "url": "https://www.semanticscholar.org/paper/15f002dde348b82817fa2a59e7ed56e6e3ec6972",
        "volume": null,
        "issue": null,
        "pages": "375-385",
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Computer Science",
          "Mathematics"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "doi:10.18653/v1/2020.emnlp-main.27"
    },
    {
      "metadata": {
        "text": "Sid Black, Gao Leo, Phil Wang, Connor Leahy, and Stella Biderman. 2021. GPT-Neo: Large Scale Autoregressive Language Modeling with MeshTensorflow. If you use this software, please cite it using these metadata.",
        "authors": [
          "Black, S.",
          "Leo, G.",
          "Wang, P.",
          "Leahy, C.",
          "Biderman, S."
        ],
        "affiliations": null,
        "title": "GPT-Neo: Large Scale Autoregressive Language Modeling with MeshTensorflow",
        "abstract": null,
        "year": 2021,
        "journal": "",
        "venue": "",
        "citation_count": 764,
        "doi": "10.5281/ZENODO.5297715",
        "backup_id": null,
        "url": "https://www.semanticscholar.org/paper/7e5008713c404445dd8786753526f1a45b93de12",
        "volume": null,
        "issue": null,
        "pages": null,
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Computer Science"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "doi:10.5281/ZENODO.5297715"
    },
    {
      "metadata": {
        "text": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877-1901. Curran Associates, Inc.",
        "authors": [
          "Brown, T.",
          "Mann, B.",
          "Ryder, N.",
          "Subbiah, M.",
          "Kaplan, J.",
          "Dhariwal, P.",
          "Neelakantan, A.",
          "Shyam, P.",
          "Sastry, G.",
          "Askell, A.",
          "Agarwal, S.",
          "Herbert-Voss, A.",
          "Krueger, G.",
          "Henighan, T.",
          "Child, R.",
          "Ramesh, A.",
          "Ziegler, D.",
          "Wu, J.",
          "Winter, C.",
          "Hesse, C.",
          "Chen, M.",
          "Sigler, E.",
          "Litwin, M.",
          "Gray, S.",
          "Chess, B.",
          "Clark, J.",
          "Berner, C.",
          "McCandlish, S.",
          "Radford, A.",
          "Sutskever, I.",
          "Amodei, D."
        ],
        "affiliations": null,
        "title": "Language models are few-shot learners",
        "abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",
        "year": 2020,
        "journal": "Advances in Neural Information Processing Systems",
        "venue": "Neural Information Processing Systems",
        "citation_count": 39976,
        "doi": null,
        "backup_id": null,
        "url": "https://www.semanticscholar.org/paper/90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
        "volume": "33",
        "issue": null,
        "pages": "1877-1901",
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Computer Science"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "title:language-models-are-few-shot-learners"
    },
    {
      "metadata": {
        "text": "Cristian Bucilua, Rich Caruana, and Alexandru Niculescu-Mizil. 2006. Model compression. In Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '06, page 535-541, New York, NY, USA. Association for Computing Machinery.",
        "authors": [
          "Bucilua, C.",
          " Caruana, R.",
          " Niculescu-Mizil, A."
        ],
        "affiliations": null,
        "title": "Model compression",
        "abstract": "The advancements in Large Language Models (LLMs) have been hindered by their substantial sizes, which necessitates LLM compression methods for practical deployment. Singular Value Decomposition (SVD) offers a promising solution for LLM compression. However, state-of-the-art SVD-based LLM compression methods have two key limitations: truncating smaller singular values may lead to higher compression loss, and the lack of update on the compressed weights after SVD truncation. In this work, we propose SVD-LLM, a SVD-based post-training LLM compression method that addresses the limitations of existing methods. SVD-LLM incorporates a truncation-aware data whitening technique to ensure a direct mapping between singular values and compression loss. Moreover, SVD-LLM adopts a parameter update with sequential low-rank approximation to compensate for the accuracy degradation after SVD compression. We evaluate SVD-LLM on 10 datasets and seven models from three different LLM families at three different scales. Our results demonstrate the superiority of SVD-LLM over state-of-the-arts, especially at high model compression ratios. Our code is available at https://github.com/AIoT-MLSys-Lab/SVD-LLM",
        "year": 2006,
        "journal": "Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
        "venue": "arXiv.org",
        "citation_count": 43,
        "doi": "10.48550/arXiv.2403.07378",
        "backup_id": null,
        "url": "https://www.semanticscholar.org/paper/4706711dc4e1e16424db9f454a1f3b092b972785",
        "volume": null,
        "issue": null,
        "pages": "535-541",
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Computer Science"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "doi:10.48550/arXiv.2403.07378"
    },
    {
      "metadata": {
        "text": "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311",
        "authors": [
          "Chowdhery, A.",
          " Narang, S.",
          " Devlin, J.",
          " Bosma, M.",
          " Mishra, G.",
          " Roberts, A.",
          " Barham, P.",
          " Chung, H.",
          " Sutton, C.",
          " Gehrmann, S.",
          " et al."
        ],
        "affiliations": null,
        "title": "Palm: Scaling language modeling with pathways",
        "abstract": "Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.",
        "year": 2022,
        "journal": "arXiv preprint arXiv",
        "venue": "Journal of machine learning research",
        "citation_count": 5997,
        "doi": null,
        "backup_id": "arXiv:2204.02311",
        "url": "https://www.semanticscholar.org/paper/094ff971d6a8b8ff870946c9b3ce5aa173617bfb",
        "volume": null,
        "issue": null,
        "pages": null,
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Computer Science"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "arXiv:2204.02311"
    },
    {
      "metadata": {
        "text": "Alice Coucke, Alaa Saade, Adrien Ball, Th\u00e9odore Bluche, Alexandre Caulier, David Leroy, Cl\u00e9ment Doumouro, Thibault Gisselbrecht, Francesco Caltagirone, Thibaut Lavril, Ma\u00ebl Primet, and Joseph Dureau. 2018. Snips voice platform: an embedded spoken language understanding system for private-by-design voice interfaces.",
        "authors": [
          "Coucke, A.",
          " Saade, A.",
          " Ball, A.",
          " Bluche, T.",
          " Caulier, A.",
          " Leroy, D.",
          " Doumouro, C.",
          " Gisselbrecht, T.",
          " Caltagirone, F.",
          " Lavril, T.",
          " Primet, M.",
          " Dureau, J."
        ],
        "affiliations": null,
        "title": "Snips voice platform: an embedded spoken language understanding system for private-by-design voice interfaces",
        "abstract": "This paper presents the machine learning architecture of the Snips Voice Platform, a software solution to perform Spoken Language Understanding on microprocessors typical of IoT devices. The embedded inference is fast and accurate while enforcing privacy by design, as no personal user data is ever collected. Focusing on Automatic Speech Recognition and Natural Language Understanding, we detail our approach to training high-performance Machine Learning models that are small enough to run in real-time on small devices. Additionally, we describe a data generation procedure that provides sufficient, high-quality training data without compromising user privacy.",
        "year": 2018,
        "journal": "ArXiv",
        "venue": "arXiv.org",
        "citation_count": 812,
        "doi": null,
        "backup_id": null,
        "url": "https://www.semanticscholar.org/paper/15c10b24ef645d83ff4059affd86945c33e00328",
        "volume": null,
        "issue": null,
        "pages": null,
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Computer Science"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "title:snips-voice-platform--an-embedded-spoken-language-understanding-system-for-private-by-design-voice-interfaces"
    },
    {
      "metadata": {
        "text": "Nicola De Cao, Gautier Izacard, Sebastian Riedel, and Fabio Petroni. 2020. Autoregressive entity retrieval.",
        "authors": [
          "De Cao, N.",
          " Izacard, G.",
          " Riedel, S.",
          " Petroni, F."
        ],
        "affiliations": null,
        "title": "Autoregressive entity retrieval",
        "abstract": "Entities are at the center of how we represent and aggregate knowledge. For instance, Encyclopedias such as Wikipedia are structured by entities (e.g., one per article). The ability to retrieve such entities given a query is fundamental for knowledge-intensive tasks such as entity linking and open-domain question answering. One way to understand current approaches is as classifiers among atomic labels, one for each entity. Their weight vectors are dense entity representations produced by encoding entity information such as descriptions. This approach leads to several shortcomings: i) context and entity affinity is mainly captured through a vector dot product, potentially missing fine-grained interactions between the two; ii) a large memory footprint is needed to store dense representations when considering large entity sets; iii) an appropriately hard set of negative data has to be subsampled at training time. We propose GENRE, the first system that retrieves entities by generating their unique names, left to right, token-by-token in an autoregressive fashion, and conditioned on the context. This enables to mitigate the aforementioned technical issues: i) the autoregressive formulation allows us to directly capture relations between context and entity name, effectively cross encoding both; ii) the memory footprint is greatly reduced because the parameters of our encoder-decoder architecture scale with vocabulary size, not entity count; iii) the exact softmax loss can be efficiently computed without the need to subsample negative data. We show the efficacy of the approach with more than 20 datasets on entity disambiguation, end-to-end entity linking and document retrieval tasks, achieving new SOTA, or very competitive results while using a tiny fraction of the memory of competing systems. Finally, we demonstrate that new entities can be added by simply specifying their unambiguous name.",
        "year": 2020,
        "journal": "ArXiv",
        "venue": "International Conference on Learning Representations",
        "citation_count": 430,
        "doi": null,
        "backup_id": null,
        "url": "https://www.semanticscholar.org/paper/572c12e81319ccd47cc0c637c82efadd03fd05ab",
        "volume": null,
        "issue": null,
        "pages": null,
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Computer Science",
          "Mathematics"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "title:autoregressive-entity-retrieval"
    },
    {
      "metadata": {
        "text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.",
        "authors": [
          "Devlin, J",
          "Chang, M-W",
          "Lee, K",
          "Toutanova, K"
        ],
        "affiliations": null,
        "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
        "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
        "year": 2019,
        "journal": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "citation_count": 92819,
        "doi": "10.18653/v1/N19-1423",
        "backup_id": null,
        "url": "https://www.semanticscholar.org/paper/df2b0e26d0599ce3e70df8a9da02e51594e0e992",
        "volume": "1",
        "issue": "1",
        "pages": "4171-4186",
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Computer Science"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "doi:10.18653/v1/N19-1423"
    },
    {
      "metadata": {
        "text": "Tommaso Furlanello, Zachary C. Lipton, Michael Tschannen, Laurent Itti, and Anima Anandkumar. 2018. Born again neural networks.",
        "authors": [
          "Furlanello, T.",
          " Lipton, Z.C.",
          " Tschannen, M.",
          " Itti, L.",
          " Anandkumar, A."
        ],
        "affiliations": null,
        "title": "Born again neural networks",
        "abstract": "Knowledge distillation (KD) consists of transferring knowledge from one machine learning model (the teacher}) to another (the student). Commonly, the teacher is a high-capacity model with formidable performance, while the student is more compact. By transferring knowledge, one hopes to benefit from the student's compactness. %we desire a compact model with performance close to the teacher's. We study KD from a new perspective: rather than compressing models, we train students parameterized identically to their teachers. Surprisingly, these {Born-Again Networks (BANs), outperform their teachers significantly, both on computer vision and language modeling tasks. Our experiments with BANs based on DenseNets demonstrate state-of-the-art performance on the CIFAR-10 (3.5%) and CIFAR-100 (15.5%) datasets, by validation error. Additional experiments explore two distillation objectives: (i) Confidence-Weighted by Teacher Max (CWTM) and (ii) Dark Knowledge with Permuted Predictions (DKPP). Both methods elucidate the essential components of KD, demonstrating a role of the teacher outputs on both predicted and non-predicted classes. We present experiments with students of various capacities, focusing on the under-explored case where students overpower teachers. Our experiments show significant advantages from transferring knowledge between DenseNets and ResNets in either direction.",
        "year": 2018,
        "journal": null,
        "venue": "International Conference on Machine Learning",
        "citation_count": 1020,
        "doi": null,
        "backup_id": null,
        "url": "https://www.semanticscholar.org/paper/2444be7584d1f5a7e2aa9f65078de09154f14ea1",
        "volume": null,
        "issue": null,
        "pages": null,
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Mathematics",
          "Computer Science"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "title:born-again-neural-networks"
    },
    {
      "metadata": {
        "text": "Chih-Wen Goo, Guang Gao, Yun-Kai Hsu, Chih-Li Hao, Tsung-Chieh Chen, Keng-Wei Hsu, and Yun-Nung Chen. 2018. Slot-gated modeling for joint slot filling and intent prediction. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 753-757, New Orleans, Louisiana. Association for Computational Linguistics.",
        "authors": [
          "Goo",
          " Gao",
          " Hsu",
          " Hao",
          " Chen",
          " Hsu",
          " Chen"
        ],
        "affiliations": null,
        "title": "Slot-gated modeling for joint slot filling and intent prediction",
        "abstract": "Attention-based recurrent neural network models for joint intent detection and slot filling have achieved the state-of-the-art performance, while they have independent attention weights. Considering that slot and intent have the strong relationship, this paper proposes a slot gate that focuses on learning the relationship between intent and slot attention vectors in order to obtain better semantic frame results by the global optimization. The experiments show that our proposed model significantly improves sentence-level semantic frame accuracy with 4.2% and 1.9% relative improvement compared to the attentional model on benchmark ATIS and Snips datasets respectively",
        "year": 2018,
        "journal": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "citation_count": 463,
        "doi": "10.18653/v1/N18-2118",
        "backup_id": null,
        "url": "https://www.semanticscholar.org/paper/903ec87f1eb8759520feecaf4a719608fa761e58",
        "volume": "2",
        "issue": "2",
        "pages": "753-757",
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Computer Science"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "doi:10.18653/v1/N18-2118"
    },
    {
      "metadata": {
        "text": "Charles T. Hemphill, John J. Godfrey, and George R. Doddington. 1990. The ATIS spoken language systems pilot corpus. In Speech and Natural Language: Proceedings of a Workshop Held at Hidden Valley, Pennsylvania, June 24-27,1990.",
        "authors": [
          "Hemphill, C. T.",
          "Godfrey, J. J.",
          "Doddington, G. R."
        ],
        "affiliations": null,
        "title": "The ATIS spoken language systems pilot corpus",
        "abstract": "Speech research has made tremendous progress in the past using the following paradigm:\u2022 define the research problem,\u2022 collect a corpus to objectively measure progress, and\u2022 solve the research problem.Natural language research, on the other hand, has typically progressed without the benefit of any corpus of data with which to test research hypotheses. We describe the Air Travel Information System (ATIS) pilot corpus, a corpus designed to measure progress in Spoken Language Systems that include both a speech and natural language component. This pilot marks the first full-scale attempt to collect such a corpus and provides guidelines for future efforts.",
        "year": 1990,
        "journal": "Speech and Natural Language: Proceedings of a Workshop Held at Hidden Valley, Pennsylvania",
        "venue": "Human Language Technology - The Baltic Perspectiv",
        "citation_count": 916,
        "doi": "10.3115/116580.116613",
        "backup_id": null,
        "url": "https://www.semanticscholar.org/paper/1d19708290ef3cc3f43c2c95b07acdd4f52f5cda",
        "volume": null,
        "issue": null,
        "pages": null,
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Computer Science"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "doi:10.3115/116580.116613"
    },
    {
      "metadata": {
        "text": "Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in a neural network.",
        "authors": [
          "Hinton, G",
          "Vinyals, O",
          "Dean, J"
        ],
        "affiliations": null,
        "title": "Distilling the knowledge in a neural network",
        "abstract": "A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.",
        "year": 2015,
        "journal": "ArXiv",
        "venue": "arXiv.org",
        "citation_count": 19245,
        "doi": null,
        "backup_id": null,
        "url": "https://www.semanticscholar.org/paper/0c908739fbff75f03469d13d4a1a07de3414ee19",
        "volume": null,
        "issue": null,
        "pages": null,
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Mathematics",
          "Computer Science"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "title:distilling-the-knowledge-in-a-neural-network"
    },
    {
      "metadata": {
        "text": "Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long short-term memory.",
        "authors": [
          "Hochreiter, S.",
          "Schmidhuber, J."
        ],
        "affiliations": null,
        "title": "Long short-term memory",
        "abstract": null,
        "year": 1997,
        "journal": "Neural Computation",
        "venue": "Neural Computation",
        "citation_count": 89470,
        "doi": "10.1162/neco.1997.9.8.1735",
        "backup_id": null,
        "url": "https://www.semanticscholar.org/paper/2e9d221c206e9503ceb452302d68d10e293f2a10",
        "volume": null,
        "issue": null,
        "pages": null,
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Computer Science",
          "Medicine"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "doi:10.1162/neco.1997.9.8.1735"
    },
    {
      "metadata": {
        "text": "Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019. Parameter-efficient transfer learning for NLP. In Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 2790-2799. PMLR.",
        "authors": [
          "Houlsby, N",
          "Giurgiu, A",
          "Jastrzebski, S",
          "Morrone, B",
          "De Laroussilhe, Q",
          "Gesmundo, A",
          "Attariyan, M",
          "Gelly, S"
        ],
        "affiliations": null,
        "title": "Parameter-efficient transfer learning for NLP",
        "abstract": "Fine-tuning large pre-trained models is an effective transfer mechanism in NLP. However, in the presence of many downstream tasks, fine-tuning is parameter inefficient: an entire new model is required for every task. As an alternative, we propose transfer with adapter modules. Adapter modules yield a compact and extensible model; they add only a few trainable parameters per task, and new tasks can be added without revisiting previous ones. The parameters of the original network remain fixed, yielding a high degree of parameter sharing. To demonstrate adapter's effectiveness, we transfer the recently proposed BERT Transformer model to 26 diverse text classification tasks, including the GLUE benchmark. Adapters attain near state-of-the-art performance, whilst adding only a few parameters per task. On GLUE, we attain within 0.4% of the performance of full fine-tuning, adding only 3.6% parameters per task. By contrast, fine-tuning trains 100% of the parameters per task.",
        "year": 2019,
        "journal": "Proceedings of the 36th International Conference on Machine Learning",
        "venue": "International Conference on Machine Learning",
        "citation_count": 4237,
        "doi": null,
        "backup_id": null,
        "url": "https://www.semanticscholar.org/paper/29ddc1f43f28af7c846515e32cc167bc66886d0c",
        "volume": "97",
        "issue": null,
        "pages": "2790-2799",
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Computer Science",
          "Mathematics"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "title:parameter-efficient-transfer-learning-for-nlp"
    },
    {
      "metadata": {
        "text": "Huggingface. Models - Hugging Face.\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. 2020. TinyBERT: Distilling BERT for natural language understanding. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 41634174, Online. Association for Computational Linguistics.",
        "authors": [
          "Jiao, X.",
          " Yin, Y.",
          " Shang, L.",
          " Jiang, X.",
          " Chen, X.",
          " Li, L.",
          " Wang, F.",
          " Liu, Q."
        ],
        "affiliations": null,
        "title": "TinyBERT: Distilling BERT for natural language understanding",
        "abstract": "Language model pre-training, such as BERT, has significantly improved the performances of many natural language processing tasks. However, pre-trained language models are usually computationally expensive, so it is difficult to efficiently execute them on resource-restricted devices. To accelerate inference and reduce model size while maintaining accuracy, we first propose a novel Transformer distillation method that is specially designed for knowledge distillation (KD) of the Transformer-based models. By leveraging this new KD method, the plenty of knowledge encoded in a large \u201cteacher\u201d BERT can be effectively transferred to a small \u201cstudent\u201d TinyBERT. Then, we introduce a new two-stage learning framework for TinyBERT, which performs Transformer distillation at both the pre-training and task-specific learning stages. This framework ensures that TinyBERT can capture the general-domain as well as the task-specific knowledge in BERT. TinyBERT4 with 4 layers is empirically effective and achieves more than 96.8% the performance of its teacher BERT-Base on GLUE benchmark, while being 7.5x smaller and 9.4x faster on inference. TinyBERT4 is also significantly better than 4-layer state-of-the-art baselines on BERT distillation, with only ~28% parameters and ~31% inference time of them. Moreover, TinyBERT6 with 6 layers performs on-par with its teacher BERT-Base.",
        "year": 2020,
        "journal": "Findings of the Association for Computational Linguistics: EMNLP 2020",
        "venue": "Findings",
        "citation_count": 1813,
        "doi": "10.18653/v1/2020.findings-emnlp.372",
        "backup_id": null,
        "url": "https://www.semanticscholar.org/paper/0cbf97173391b0430140117027edcaf1a37968c7",
        "volume": null,
        "issue": null,
        "pages": "41634174",
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Computer Science"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "doi:10.18653/v1/2020.findings-emnlp.372"
    },
    {
      "metadata": {
        "text": "Haoran Li, Abhinav Arora, Shuohui Chen, Anchit Gupta, Sonal Gupta, and Yashar Mehdad. 2021. MTOP: A comprehensive multilingual task-oriented semantic parsing benchmark. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 2950-2962, Online. Association for Computational Linguistics.",
        "authors": [
          "Li, H.",
          " Arora, A.",
          " Chen, S.",
          " Gupta, A.",
          " Gupta, S.",
          " Mehdad, Y."
        ],
        "affiliations": null,
        "title": "MTOP: A comprehensive multilingual task-oriented semantic parsing benchmark",
        "abstract": "Scaling semantic parsing models for task-oriented dialog systems to new languages is often expensive and time-consuming due to the lack of available datasets. Available datasets suffer from several shortcomings: a) they contain few languages b) they contain small amounts of labeled examples per language c) they are based on the simple intent and slot detection paradigm for non-compositional queries. In this paper, we present a new multilingual dataset, called MTOP, comprising of 100k annotated utterances in 6 languages across 11 domains. We use this dataset and other publicly available datasets to conduct a comprehensive benchmarking study on using various state-of-the-art multilingual pre-trained models for task-oriented semantic parsing. We achieve an average improvement of +6.3 points on Slot F1 for the two existing multilingual datasets, over best results reported in their experiments. Furthermore, we demonstrate strong zero-shot performance using pre-trained models combined with automatic translation and alignment, and a proposed distant supervision method to reduce the noise in slot label projection.",
        "year": 2021,
        "journal": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
        "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
        "citation_count": 171,
        "doi": "10.18653/v1/2021.eacl-main.257",
        "backup_id": null,
        "url": "Online",
        "volume": null,
        "issue": null,
        "pages": "2950-2962",
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Computer Science"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "doi:10.18653/v1/2021.eacl-main.257"
    },
    {
      "metadata": {
        "text": "Ilya Loshchilov and Frank Hutter. 2017. Fixing weight decay regularization in adam. ArXiv, abs/1711.05101",
        "authors": [
          "Loshchilov, I.",
          " Hutter, F."
        ],
        "affiliations": null,
        "title": "Fixing weight decay regularization in adam",
        "abstract": "We note that common implementations of adaptive gradient algorithms, such as Adam, limit the potential benefit of weight decay regularization, because the weights do not decay multiplicatively (as would be expected for standard weight decay) but by an additive constant factor. We propose a simple way to resolve this issue by decoupling weight decay and the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam, and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). We also demonstrate that longer optimization runs require smaller weight decay values for optimal results and introduce a normalized variant of weight decay to reduce this dependence. Finally, we propose a version of Adam with warm restarts (AdamWR) that has strong anytime performance while achieving state-of-the-art results on CIFAR-10 and ImageNet32x32. Our source code will become available after the review process.",
        "year": 2017,
        "journal": "ArXiv",
        "venue": "arXiv.org",
        "citation_count": 2078,
        "doi": null,
        "backup_id": "abs/1711.05101",
        "url": "https://www.semanticscholar.org/paper/45dfef0cc1ed96558c1c650432ce39d6a1050b6a",
        "volume": null,
        "issue": null,
        "pages": null,
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Mathematics",
          "Computer Science"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "abs/1711.05101"
    },
    {
      "metadata": {
        "text": "Subhabrata Mukherjee and Ahmed Hassan Awadallah. 2020. XtremeDistil: Multi-stage distillation for massive multilingual models. In Proceedings of the 58th Annual Meeting of the Association for Computational",
        "authors": [
          "Mukherjee, S.",
          " Awadallah, A. H."
        ],
        "affiliations": null,
        "title": "XtremeDistil: Multi-stage distillation for massive multilingual models",
        "abstract": "Deep and large pre-trained language models are the state-of-the-art for various natural language processing tasks. However, the huge size of these models could be a deterrent to using them in practice. Some recent works use knowledge distillation to compress these huge models into shallow ones. In this work we study knowledge distillation with a focus on multilingual Named Entity Recognition (NER). In particular, we study several distillation strategies and propose a stage-wise optimization scheme leveraging teacher internal representations, that is agnostic of teacher architecture, and show that it outperforms strategies employed in prior works. Additionally, we investigate the role of several factors like the amount of unlabeled data, annotation resources, model architecture and inference latency to name a few. We show that our approach leads to massive compression of teacher models like mBERT by upto 35x in terms of parameters and 51x in terms of latency for batch inference while retaining 95% of its F1-score for NER over 41 languages.",
        "year": 2020,
        "journal": "Proceedings of the 58th Annual Meeting of the Association for Computational",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citation_count": 56,
        "doi": "10.18653/v1/2020.acl-main.202",
        "backup_id": null,
        "url": "https://www.semanticscholar.org/paper/9c5a239b75bade55c830b164e2fadc424e879137",
        "volume": null,
        "issue": null,
        "pages": null,
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Computer Science"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "doi:10.18653/v1/2020.acl-main.202"
    },
    {
      "metadata": {
        "text": "Linguistics, pages 2221-2234, Online. Association for Computational Linguistics.",
        "authors": null,
        "affiliations": null,
        "title": null,
        "abstract": null,
        "year": null,
        "journal": "Linguistics",
        "venue": null,
        "citation_count": null,
        "doi": null,
        "backup_id": null,
        "url": null,
        "volume": null,
        "issue": null,
        "pages": "2221-2234",
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": null,
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "title:linguistics--pages-2221-2234--online--association-for-computational-linguistics-"
    },
    {
      "metadata": {
        "text": "Giovanni Paolini, Ben Athiwaratkun, Jason Krone, Jie Ma, Alessandro Achille, Rishita Anubhai, Cicero Nogueira dos Santos, Bing Xiang, and Stefano Soatto. 2021. Structured prediction as translation between augmented natural languages.",
        "authors": [
          "Paolini, G.",
          " Athiwaratkun, B.",
          " Krone, J.",
          " Ma, J.",
          " Achille, A.",
          " Anubhai, R.",
          " dos Santos, C. N.",
          " Xiang, B.",
          " Soatto, S."
        ],
        "affiliations": null,
        "title": "Structured prediction as translation between augmented natural languages",
        "abstract": "We propose a new framework, Translation between Augmented Natural Languages (TANL), to solve many structured prediction language tasks including joint entity and relation extraction, nested named entity recognition, relation classification, semantic role labeling, event extraction, coreference resolution, and dialogue state tracking. Instead of tackling the problem by training task-specific discriminative classifiers, we frame it as a translation task between augmented natural languages, from which the task-relevant information can be easily extracted. Our approach can match or outperform task-specific models on all tasks, and in particular, achieves new state-of-the-art results on joint entity and relation extraction (CoNLL04, ADE, NYT, and ACE2005 datasets), relation classification (FewRel and TACRED), and semantic role labeling (CoNLL-2005 and CoNLL-2012). We accomplish this while using the same architecture and hyperparameters for all tasks and even when training a single model to solve all tasks at the same time (multi-task learning). Finally, we show that our framework can also significantly improve the performance in a low-resource regime, thanks to better use of label semantics.",
        "year": 2021,
        "journal": "ArXiv",
        "venue": "International Conference on Learning Representations",
        "citation_count": 284,
        "doi": null,
        "backup_id": null,
        "url": "https://www.semanticscholar.org/paper/1cb3f6d545b68db3e7fc6055dcf44099c3ac4672",
        "volume": null,
        "issue": null,
        "pages": null,
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Computer Science"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "title:structured-prediction-as-translation-between-augmented-natural-languages"
    },
    {
      "metadata": {
        "text": "StanfordNLP. GloVe: Global Vectors for Word Representation",
        "authors": [
          "Jeffrey Pennington",
          "R. Socher",
          "Christopher D. Manning"
        ],
        "affiliations": null,
        "title": "GloVe: Global Vectors for Word Representation",
        "abstract": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.",
        "year": 2014,
        "journal": null,
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citation_count": 31881,
        "doi": "10.3115/v1/D14-1162",
        "backup_id": null,
        "url": "https://www.semanticscholar.org/paper/f37e1b62a767a307c046404ca96bc140b3e68cb5",
        "volume": null,
        "issue": null,
        "pages": null,
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Computer Science"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "doi:10.3115/v1/D14-1162"
    },
    {
      "metadata": {
        "text": "Chengwei Qin and Shafiq Joty. 2021. Lfpt5: A unified framework for lifelong few-shot language learning based on prompt tuning of t 5 .",
        "authors": [
          "Qin, C",
          " Joty, S"
        ],
        "affiliations": null,
        "title": "Lfpt5: A unified framework for lifelong few-shot language learning based on prompt tuning of t 5",
        "abstract": "Existing approaches to lifelong language learning rely on plenty of labeled data for learning a new task, which is hard to obtain in most real scenarios. Considering that humans can continually learn new tasks from a handful of examples, we expect the models also to be able to generalize well on new few-shot tasks without forgetting the previous ones. In this work, we define this more challenging yet practical problem as Lifelong Few-shot Language Learning (LFLL) and propose a unified framework for it based on prompt tuning of T5. Our framework called LFPT5 takes full advantage of PT's strong few-shot learning ability, and simultaneously trains the model as a task solver and a data generator. Before learning a new domain of the same task type, LFPT5 generates pseudo (labeled) samples of previously learned domains, and later gets trained on those samples to alleviate forgetting of previous knowledge as it learns the new domain. In addition, a KL divergence loss is minimized to achieve label consistency between the previous and the current model. While adapting to a new task type, LFPT5 includes and tunes additional prompt embeddings for the new task. With extensive experiments, we demonstrate that LFPT5 can be applied to various different types of tasks and significantly outperform previous methods in different LFLL settings.",
        "year": 2021,
        "journal": "ArXiv",
        "venue": "International Conference on Learning Representations",
        "citation_count": 98,
        "doi": null,
        "backup_id": null,
        "url": "https://www.semanticscholar.org/paper/fa133b4200729a57db96ae50aff8c4a5ff819f43",
        "volume": null,
        "issue": null,
        "pages": null,
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Computer Science"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "title:lfpt5--a-unified-framework-for-lifelong-few-shot-language-learning-based-on-prompt-tuning-of-t-5"
    },
    {
      "metadata": {
        "text": "Alec Radford, Karthik Narasimhan, and Tim Salimansand Ilya Sutskever. 2018. Improving language understanding by generative pre-training. https: //www.cs.ubc.ca/ amuham01/LING530/ papers/radford2018improving.pdf",
        "authors": [
          "Radford, A.",
          "Narasimhan, K.",
          "Salimans, T.",
          "Sutskever, I."
        ],
        "affiliations": null,
        "title": "Improving language understanding by generative pre-training",
        "abstract": null,
        "year": 2018,
        "journal": "",
        "venue": "",
        "citation_count": 11631,
        "doi": null,
        "backup_id": null,
        "url": "https://www.cs.ubc.ca/amuham01/LING530/papers/radford2018improving.pdf",
        "volume": null,
        "issue": null,
        "pages": null,
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Computer Science"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "title:improving-language-understanding-by-generative-pre-training"
    },
    {
      "metadata": {
        "text": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Stuskever. 2019. Language models are unsupervised multitask learners. Technical report, OpenAI.",
        "authors": [
          "Radford, A",
          "Wu, J",
          "Child, R",
          "Luan, D",
          "Amodei, D",
          "Stuskever, I"
        ],
        "affiliations": null,
        "title": "Language models are unsupervised multitask learners",
        "abstract": null,
        "year": 2019,
        "journal": "Technical report, OpenAI",
        "venue": "",
        "citation_count": 22153,
        "doi": null,
        "backup_id": null,
        "url": "https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe",
        "volume": null,
        "issue": null,
        "pages": null,
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Computer Science"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "title:language-models-are-unsupervised-multitask-learners"
    },
    {
      "metadata": {
        "text": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1-67.",
        "authors": [
          "Raffel, C.",
          "Shazeer, N.",
          "Roberts, A.",
          "Lee, K.",
          "Narang, S.",
          "Matena, M.",
          "Zhou, Y.",
          "Li, W.",
          "Liu, P. J."
        ],
        "affiliations": null,
        "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
        "abstract": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.",
        "year": 2020,
        "journal": "Journal of Machine Learning Research",
        "venue": "Journal of machine learning research",
        "citation_count": 19422,
        "doi": null,
        "backup_id": null,
        "url": "https://www.semanticscholar.org/paper/6c4b76232bb72897685d19b3d264c6ee3005bc2b",
        "volume": "21",
        "issue": "140",
        "pages": "1-67",
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Mathematics",
          "Computer Science"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "title:exploring-the-limits-of-transfer-learning-with-a-unified-text-to-text-transformer"
    },
    {
      "metadata": {
        "text": "Karthik Raman, Iftekhar Naim, Jiecao Chen, Kazuma Hashimoto, Kiran Yalasangi, and Krishna Srinivasan. 2022. Transforming sequence tagging into a seq2seq task.",
        "authors": [
          "Raman, K.",
          " Naim, I.",
          " Chen, J.",
          " Hashimoto, K.",
          " Yalasangi, K.",
          " Srinivasan, K."
        ],
        "affiliations": null,
        "title": "Transforming sequence tagging into a seq2seq task",
        "abstract": "Pretrained, large, generative language models (LMs) have had great success in a wide range of sequence tagging and structured prediction tasks. Casting a sequence tagging task as a Seq2Seq one requires deciding the formats of the input and output sequences. However, we lack a principled understanding of the trade-offs associated with these formats (such as the effect on model accuracy, sequence length, multilingual generalization, hallucination). In this paper, we rigorously study different formats one could use for casting input text sentences and their output labels into the input and target (i.e., output) of a Seq2Seq model. Along the way, we introduce a new format, which we show to to be both simpler and more effective. Additionally the new format demonstrates significant gains in the multilingual settings \u2013 both zero-shot transfer learning and joint training. Lastly, we find that the new format is more robust and almost completely devoid of hallucination \u2013 an issue we find common in existing formats. With well over a 1000 experiments studying 14 different formats, over 7 diverse public benchmarks \u2013 including 3 multilingual datasets spanning 7 languages \u2013 we believe our findings provide a strong empirical basis in understanding how we should tackle sequence tagging tasks.",
        "year": 2022,
        "journal": "ArXiv",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citation_count": 23,
        "doi": "10.48550/arXiv.2203.08378",
        "backup_id": null,
        "url": "https://www.semanticscholar.org/paper/f35dbee22c1572d149b7c1e20d69672cae931451",
        "volume": null,
        "issue": null,
        "pages": null,
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Computer Science"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "doi:10.48550/arXiv.2203.08378"
    },
    {
      "metadata": {
        "text": "Sebastian Schuster, Sonal Gupta, Rushin Shah, and Mike Lewis. 2019. Cross-lingual transfer learning for multilingual task oriented dialog. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3795-3805, Minneapolis, Minnesota. Association for Computational Linguistics.",
        "authors": [
          "Schuster, S.",
          " Gupta, S.",
          " Shah, R.",
          " Lewis, M."
        ],
        "affiliations": null,
        "title": "Cross-lingual transfer learning for multilingual task oriented dialog",
        "abstract": "One of the first steps in the utterance interpretation pipeline of many task-oriented conversational AI systems is to identify user intents and the corresponding slots. Since data collection for machine learning models for this task is time-consuming, it is desirable to make use of existing data in a high-resource language to train models in low-resource languages. However, development of such models has largely been hindered by the lack of multilingual training data. In this paper, we present a new data set of 57k annotated utterances in English (43k), Spanish (8.6k) and Thai (5k) across the domains weather, alarm, and reminder. We use this data set to evaluate three different cross-lingual transfer methods: (1) translating the training data, (2) using cross-lingual pre-trained embeddings, and (3) a novel method of using a multilingual machine translation encoder as contextual word representations. We find that given several hundred training examples in the the target language, the latter two methods outperform translating the training data. Further, in very low-resource settings, multilingual contextual word representations give better results than using cross-lingual static embeddings. We also compare the cross-lingual methods to using monolingual resources in the form of contextual ELMo representations and find that given just small amounts of target language data, this method outperforms all cross-lingual methods, which highlights the need for more sophisticated cross-lingual methods.",
        "year": 2019,
        "journal": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "citation_count": 282,
        "doi": "10.18653/v1/N19-1380",
        "backup_id": null,
        "url": "https://www.semanticscholar.org/paper/c644956d5cfdb7ad7ea24a420608b9b58c148e3d",
        "volume": "1",
        "issue": "1",
        "pages": "3795-3805",
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Computer Science"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "doi:10.18653/v1/N19-1380"
    },
    {
      "metadata": {
        "text": "Sam Shleifer and Alexander M. Rush. 2020. Pre-trained summarization distillation.",
        "authors": [
          "Shleifer, S.",
          "Rush, A. M."
        ],
        "affiliations": null,
        "title": "Pre-trained summarization distillation",
        "abstract": "Recent state-of-the-art approaches to summarization utilize large pre-trained Transformer models. Distilling these models to smaller student models has become critically important for practical use; however there are many different distillation methods proposed by the NLP literature. Recent work on distilling BERT for classification and regression tasks shows strong performance using direct knowledge distillation. Alternatively, machine translation practitioners distill using pseudo-labeling, where a small model is trained on the translations of a larger model. A third, simpler approach is to 'shrink and fine-tune' (SFT), which avoids any explicit distillation by copying parameters to a smaller student model and then fine-tuning. We compare these three approaches for distillation of Pegasus and BART, the current and former state of the art, pre-trained summarization models, and find that SFT outperforms knowledge distillation and pseudo-labeling on the CNN/DailyMail dataset, but under-performs pseudo-labeling on the more abstractive XSUM dataset. PyTorch Code and checkpoints of different sizes are available through Hugging Face transformers here this http URL.",
        "year": 2020,
        "journal": "ArXiv",
        "venue": "arXiv.org",
        "citation_count": 98,
        "doi": null,
        "backup_id": null,
        "url": "https://www.semanticscholar.org/paper/6044e943a7f7e8741431028fdbdaf63754cd8d5f",
        "volume": null,
        "issue": null,
        "pages": null,
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Computer Science"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "title:pre-trained-summarization-distillation"
    },
    {
      "metadata": {
        "text": "Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova, and Jimmy Lin. 2019. Distilling taskspecific knowledge from bert into simple neural networks.",
        "authors": [
          "Tang, R.",
          " Lu, Y.",
          " Liu, L.",
          " Mou, L.",
          " Vechtomova, O.",
          " Lin, J."
        ],
        "affiliations": null,
        "title": "Distilling taskspecific knowledge from bert into simple neural networks",
        "abstract": "In the natural language processing literature, neural networks are becoming increasingly deeper and complex. The recent poster child of this trend is the deep language representation model, which includes BERT, ELMo, and GPT. These developments have led to the conviction that previous-generation, shallower neural networks for language understanding are obsolete. In this paper, however, we demonstrate that rudimentary, lightweight neural networks can still be made competitive without architecture changes, external training data, or additional input features. We propose to distill knowledge from BERT, a state-of-the-art language representation model, into a single-layer BiLSTM, as well as its siamese counterpart for sentence-pair tasks. Across multiple datasets in paraphrasing, natural language inference, and sentiment classification, we achieve comparable results with ELMo, while using roughly 100 times fewer parameters and 15 times less inference time.",
        "year": 2019,
        "journal": "ArXiv",
        "venue": "arXiv.org",
        "citation_count": 417,
        "doi": null,
        "backup_id": null,
        "url": "https://www.semanticscholar.org/paper/a08293b2c9c5bcddb023cc7eb3354d4d86bfae89",
        "volume": null,
        "issue": null,
        "pages": null,
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Computer Science"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "title:distilling-taskspecific-knowledge-from-bert-into-simple-neural-networks"
    },
    {
      "metadata": {
        "text": "Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the CoNLL-2003 shared task: Languageindependent named entity recognition. In Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003, pages 142-147.",
        "authors": [
          "Tjong Kim Sang, E.F.",
          "De Meulder, F."
        ],
        "affiliations": null,
        "title": "Introduction to the CoNLL-2003 shared task: Languageindependent named entity recognition",
        "abstract": "We describe the CoNLL-2003 shared task: language-independent named entity recognition. We give background information on the data sets (English and German) and the evaluation method, present a general overview of the systems that have taken part in the task and discuss their performance.",
        "year": 2003,
        "journal": "Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003",
        "venue": "Conference on Computational Natural Language Learning",
        "citation_count": 4302,
        "doi": "10.3115/1119176.1119195",
        "backup_id": null,
        "url": "https://www.semanticscholar.org/paper/10f97f1fb4f5c2c8e6c44d4a33da46d331dd4aeb",
        "volume": null,
        "issue": null,
        "pages": "142-147",
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Computer Science"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "doi:10.3115/1119176.1119195"
    },
    {
      "metadata": {
        "text": "Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003, pages 142147",
        "authors": null,
        "affiliations": null,
        "title": null,
        "abstract": null,
        "year": 2003,
        "journal": "Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL",
        "venue": null,
        "citation_count": null,
        "doi": null,
        "backup_id": null,
        "url": null,
        "volume": null,
        "issue": null,
        "pages": "142-147",
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": null,
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "title:proceedings-of-the-seventh-conference-on-natural-language-learning-at-hlt-naacl-2003--pages-142147"
    },
    {
      "metadata": {
        "text": "Iulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Well-read students learn better: On the importance of pre-training compact models.",
        "authors": [
          "Turc, I.",
          " Chang, M.-W.",
          " Lee, K.",
          " Toutanova, K."
        ],
        "affiliations": null,
        "title": "Well-read students learn better: On the importance of pre-training compact models",
        "abstract": null,
        "year": 2019,
        "journal": "arXiv: Computation and Language",
        "venue": "",
        "citation_count": 650,
        "doi": null,
        "backup_id": null,
        "url": "https://www.semanticscholar.org/paper/7402b604f14b8b91c53ed6eed04af92c59636c97",
        "volume": null,
        "issue": null,
        "pages": null,
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Computer Science"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "title:well-read-students-learn-better--on-the-importance-of-pre-training-compact-models"
    },
    {
      "metadata": {
        "text": "Linting Xue, Aditya Barua, Noah Constant, Rami AlRfou, Sharan Narang, Mihir Kale, Adam Roberts, and Colin Raffel. 2022. Byt5: Towards a token-free future with pre-trained byte-to-byte models. Transactions of the Association for Computational Linguistics, 10:291-306.",
        "authors": [
          "Linting Xue",
          " Aditya Barua",
          " Noah Constant",
          " Rami AlRfou",
          " Sharan Narang",
          " Mihir Kale",
          " Adam Roberts",
          " Colin Raffel"
        ],
        "affiliations": null,
        "title": "Byt5: Towards a token-free future with pre-trained byte-to-byte models",
        "abstract": "Most widely used pre-trained language models operate on sequences of tokens corresponding to word or subword units. By comparison, token-free models that operate directly on raw text (bytes or characters) have many benefits: They can process text in any language out of the box, they are more robust to noise, and they minimize technical debt by removing complex and error-prone text preprocessing pipelines. Because byte or character sequences are longer than token sequences, past work on token-free models has often introduced new model architectures designed to amortize the cost of operating directly on raw text. In this paper, we show that a standard Transformer architecture can be used with minimal modifications to process byte sequences. We characterize the trade-offs in terms of parameter count, training FLOPs, and inference speed, and show that byte-level models are competitive with their token-level counterparts. We also demonstrate that byte-level models are significantly more robust to noise and perform better on tasks that are sensitive to spelling and pronunciation. As part of our contribution, we release a new set of pre-trained byte-level Transformer models based on the T5 architecture, as well as all code and data used in our experiments.1",
        "year": 2022,
        "journal": "Transactions of the Association for Computational Linguistics",
        "venue": "Transactions of the Association for Computational Linguistics",
        "citation_count": 464,
        "doi": "10.1162/tacl_a_00461",
        "backup_id": null,
        "url": "https://www.semanticscholar.org/paper/1006d191e9eb5b4dbc35fc0bb389328ddc75cba7",
        "volume": null,
        "issue": "10",
        "pages": "291-306",
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Computer Science"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "doi:10.1162/tacl_a_00461"
    },
    {
      "metadata": {
        "text": "Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021. mT5: A massively multilingual pre-trained text-to-text transformer. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 483-498, Online. Association for Computational Linguistics.",
        "authors": [
          "Linting Xue",
          " Noah Constant",
          " Adam Roberts",
          " Mihir Kale",
          " Rami Al-Rfou",
          " Aditya Siddhant",
          " Aditya Barua",
          " Colin Raffel"
        ],
        "affiliations": null,
        "title": "mT5: A massively multilingual pre-trained text-to-text transformer",
        "abstract": "The recent \u201cText-to-Text Transfer Transformer\u201d (T5) leveraged a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual benchmarks. We also describe a simple technique to prevent \u201caccidental translation\u201d in the zero-shot setting, where a generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model checkpoints used in this work are publicly available.",
        "year": 2021,
        "journal": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "citation_count": 2440,
        "doi": "10.18653/V1/2021.NAACL-MAIN.41",
        "backup_id": null,
        "url": "Online",
        "volume": null,
        "issue": null,
        "pages": "483-498",
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Computer Science"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "doi:10.18653/V1/2021.NAACL-MAIN.41"
    },
    {
      "metadata": {
        "text": "Hang Yan, Tao Gui, Junqi Dai, Qipeng Guo, Zheng Zhang, and Xipeng Qiu. 2021. A unified generative framework for various NER subtasks. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 5808-5822, Online. Association for Computational Linguistics.",
        "authors": [
          "Yan, H.",
          "Gui, T.",
          "Dai, J.",
          "Guo, Q.",
          "Zhang, Z.",
          "Qiu, X."
        ],
        "affiliations": null,
        "title": "A unified generative framework for various NER subtasks",
        "abstract": "Named Entity Recognition (NER) is the task of identifying spans that represent entities in sentences. Whether the entity spans are nested or discontinuous, the NER task can be categorized into the flat NER, nested NER, and discontinuous NER subtasks. These subtasks have been mainly solved by the token-level sequence labelling or span-level classification. However, these solutions can hardly tackle the three kinds of NER subtasks concurrently. To that end, we propose to formulate the NER subtasks as an entity span sequence generation task, which can be solved by a unified sequence-to-sequence (Seq2Seq) framework. Based on our unified framework, we can leverage the pre-trained Seq2Seq model to solve all three kinds of NER subtasks without the special design of the tagging schema or ways to enumerate spans. We exploit three types of entity representations to linearize entities into a sequence. Our proposed framework is easy-to-implement and achieves state-of-the-art (SoTA) or near SoTA performance on eight English NER datasets, including two flat NER datasets, three nested NER datasets, and three discontinuous NER datasets.",
        "year": 2021,
        "journal": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citation_count": 288,
        "doi": "10.18653/v1/2021.acl-long.451",
        "backup_id": null,
        "url": "Online",
        "volume": null,
        "issue": null,
        "pages": "5808-5822",
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Computer Science"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "doi:10.18653/v1/2021.acl-long.451"
    },
    {
      "metadata": {
        "text": "Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. Opt: Open pretrained transformer language models.",
        "authors": [
          "Zhang, S.",
          " Roller, S.",
          " Goyal, N.",
          " Artetxe, M.",
          " Chen, M.",
          " Chen, S.",
          " Dewan, C.",
          " Diab, M.",
          " Li, X.",
          " Lin, X.",
          " Mihaylov, T.",
          " Ott, M.",
          " Shleifer, S.",
          " Shuster, K.",
          " Simig, D.",
          " Singh Koura, P.",
          " Sridhar, A.",
          " Wang, T.",
          " Zettlemoyer, L."
        ],
        "affiliations": null,
        "title": "Opt: Open pretrained transformer language models",
        "abstract": "We show for the first time that large-scale generative pretrained transformer (GPT) family models can be pruned to at least 50% sparsity in one-shot, without any retraining, at minimal loss of accuracy. This is achieved via a new pruning method called SparseGPT, specifically designed to work efficiently and accurately on massive GPT-family models. We can execute SparseGPT on the largest available open-source models, OPT-175B and BLOOM-176B, in under 4.5 hours, and can reach 60% unstructured sparsity with negligible increase in perplexity: remarkably, more than 100 billion weights from these models can be ignored at inference time. SparseGPT generalizes to semi-structured (2:4 and 4:8) patterns, and is compatible with weight quantization approaches. The code is available at: https://github.com/IST-DASLab/sparsegpt.",
        "year": 2022,
        "journal": "ArXiv",
        "venue": "International Conference on Machine Learning",
        "citation_count": 624,
        "doi": "10.48550/arXiv.2301.00774",
        "backup_id": null,
        "url": "https://www.semanticscholar.org/paper/909ad57ce8caa6b390a65ae09db352d27d8f3996",
        "volume": null,
        "issue": null,
        "pages": null,
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Computer Science"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "doi:10.48550/arXiv.2301.00774"
    },
    {
      "metadata": {
        "text": "Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, and Chris Dyer. 2016. Neural architectures for named entity recognition.",
        "authors": [
          "Lample, G.",
          "Ballesteros, M.",
          "Subramanian, S.",
          "Kawakami, K.",
          "Dyer, C."
        ],
        "affiliations": null,
        "title": "Neural architectures for named entity recognition",
        "abstract": "Comunicacio presentada a la 2016 Conference of the North American Chapter of the Association for Computational Linguistics, celebrada a San Diego (CA, EUA) els dies 12 a 17 de juny 2016.",
        "year": 2016,
        "journal": null,
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "citation_count": 3990,
        "doi": "10.18653/v1/N16-1030",
        "backup_id": null,
        "url": "https://www.semanticscholar.org/paper/f5a7da72496e2ca8edcd9f9123773012c010cfc6",
        "volume": null,
        "issue": null,
        "pages": null,
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Computer Science"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "doi:10.18653/v1/N16-1030"
    },
    {
      "metadata": {
        "text": "Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova, and Jimmy Lin. 2019. Distilling task-specific knowledge from bert into simple neural networks.",
        "authors": [
          "Tang, R.",
          " Lu, Y.",
          " Liu, L.",
          " Mou, L.",
          " Vechtomova, O.",
          " Lin, J."
        ],
        "affiliations": null,
        "title": "Distilling task-specific knowledge from bert into simple neural networks",
        "abstract": "In the natural language processing literature, neural networks are becoming increasingly deeper and complex. The recent poster child of this trend is the deep language representation model, which includes BERT, ELMo, and GPT. These developments have led to the conviction that previous-generation, shallower neural networks for language understanding are obsolete. In this paper, however, we demonstrate that rudimentary, lightweight neural networks can still be made competitive without architecture changes, external training data, or additional input features. We propose to distill knowledge from BERT, a state-of-the-art language representation model, into a single-layer BiLSTM, as well as its siamese counterpart for sentence-pair tasks. Across multiple datasets in paraphrasing, natural language inference, and sentiment classification, we achieve comparable results with ELMo, while using roughly 100 times fewer parameters and 15 times less inference time.",
        "year": 2019,
        "journal": "ArXiv",
        "venue": "arXiv.org",
        "citation_count": 417,
        "doi": null,
        "backup_id": null,
        "url": "https://www.semanticscholar.org/paper/a08293b2c9c5bcddb023cc7eb3354d4d86bfae89",
        "volume": null,
        "issue": null,
        "pages": null,
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Computer Science"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "title:distilling-task-specific-knowledge-from-bert-into-simple-neural-networks"
    },
    {
      "metadata": {
        "text": null,
        "authors": [
          "Bhowmik, R.",
          " Jiang, R.",
          " Ponza, M.",
          " Tendle, A.",
          " Gupta, A.",
          " Lu, X.",
          " Zhao, Q.",
          " Preo\u0163iuc-Pietro, D."
        ],
        "affiliations": [
          "Bloomberg"
        ],
        "title": "Leveraging Contextual Information for Effective Entity Salience Detection",
        "abstract": "In text documents such as news articles, the content and key events usually revolve around a subset of all the entities mentioned in a document. These entities, often deemed as salient entities, provide useful cues of the aboutness of a document to a reader. Identifying the salience of entities was found helpful in several downstream applications such as search, ranking, and entity-centric summarization, among others. Prior work on salient entity detection mainly focused on machine learning models that require heavy feature engineering. We show that fine-tuning medium-sized language models with a cross-encoder style architecture yields substantial performance gains over feature engineering approaches. To this end, we conduct a comprehensive benchmarking of four publicly available datasets using models representative of the medium-sized pre-trained language model family. Additionally, we show that zero-shot prompting of instruction-tuned language models yields inferior results, indicating the task's uniqueness and complexity.",
        "year": 2023,
        "journal": null,
        "venue": "NAACL-HLT",
        "citation_count": 1,
        "doi": "10.48550/arXiv.2309.07990",
        "backup_id": null,
        "url": "https://www.semanticscholar.org/paper/8a655a1b1deac0ba8792c4538b69f828983e363a",
        "volume": null,
        "issue": null,
        "pages": null,
        "is_document_citation": true,
        "formatted": null,
        "fields_of_study": [
          "Computer Science"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "doi:10.48550/arXiv.2309.07990"
    },
    {
      "metadata": {
        "text": "Alan Akbik, Tanja Bergmann, Duncan Blythe, Kashif Rasul, Stefan Schweter, and Roland Vollgraf. 2019. FLAIR: An easy-to-use framework for state-of-the-art NLP. In NAACL 2019, 2019 Annual Conference of the North American Chapter of the Association for",
        "authors": [
          "Akbik, A.",
          " Bergmann, T.",
          " Blythe, D.",
          " Rasul, K.",
          " Schweter, S.",
          " Vollgraf, R."
        ],
        "affiliations": null,
        "title": "FLAIR: An easy-to-use framework for state-of-the-art NLP",
        "abstract": "We present FLAIR, an NLP framework designed to facilitate training and distribution of state-of-the-art sequence labeling, text classification and language models. The core idea of the framework is to present a simple, unified interface for conceptually very different types of word and document embeddings. This effectively hides all embedding-specific engineering complexity and allows researchers to \u201cmix and match\u201d various embeddings with little effort. The framework also implements standard model training and hyperparameter selection routines, as well as a data fetching module that can download publicly available NLP datasets and convert them into data structures for quick set up of experiments. Finally, FLAIR also ships with a \u201cmodel zoo\u201d of pre-trained models to allow researchers to use state-of-the-art NLP models in their applications. This paper gives an overview of the framework and its functionality. The framework is available on GitHub at https://github.com/zalandoresearch/flair .",
        "year": 2019,
        "journal": "NAACL 2019, 2019 Annual Conference of the North American Chapter of the Association for",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "citation_count": 795,
        "doi": "10.18653/v1/N19-4010",
        "backup_id": null,
        "url": "https://www.semanticscholar.org/paper/3a7bbc46795929f0eace82b64c44c92a48682fb5",
        "volume": null,
        "issue": null,
        "pages": null,
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Computer Science"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "doi:10.18653/v1/N19-4010"
    },
    {
      "metadata": {
        "text": "Computational Linguistics (Demonstrations), pages 54-59.",
        "authors": null,
        "affiliations": null,
        "title": null,
        "abstract": null,
        "year": null,
        "journal": "Computational Linguistics",
        "venue": null,
        "citation_count": null,
        "doi": null,
        "backup_id": null,
        "url": null,
        "volume": null,
        "issue": null,
        "pages": "54-59",
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": null,
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "title:computational-linguistics--demonstrations---pages-54-59-"
    },
    {
      "metadata": {
        "text": "Zakariae Alami Merrouni, Bouchra Frikh, and Brahim Oubbi. 2020. Automatic keyphrase extraction: a survey and trends. Journal of Intelligent Information Systems, 54(2):391-424.",
        "authors": [
          "Alami Merrouni, Zakariae",
          "Frikh, Bouchra",
          "Oubbi, Brahim"
        ],
        "affiliations": null,
        "title": "Automatic keyphrase extraction: a survey and trends",
        "abstract": null,
        "year": 2020,
        "journal": "Journal of Intelligent Information Systems",
        "venue": "Journal of Intelligence and Information Systems",
        "citation_count": 86,
        "doi": "10.1007/s10844-019-00558-9",
        "backup_id": null,
        "url": "https://www.semanticscholar.org/paper/ac8fc73c057f34ac2cffce415faced698e0c37f0",
        "volume": "54",
        "issue": "2",
        "pages": "391-424",
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Computer Science"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "doi:10.1007/s10844-019-00558-9"
    },
    {
      "metadata": {
        "text": "Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The long-document transformer.",
        "authors": [
          "Iz Beltagy",
          "Matthew E. Peters",
          "Arman Cohan"
        ],
        "affiliations": null,
        "title": "Longformer: The long-document transformer",
        "abstract": "Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a Longformer variant for supporting long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization dataset.",
        "year": 2020,
        "journal": "ArXiv",
        "venue": "arXiv.org",
        "citation_count": 3912,
        "doi": null,
        "backup_id": null,
        "url": "https://www.semanticscholar.org/paper/925ad2897d1b5decbea320d07e99afa9110e09b2",
        "volume": null,
        "issue": null,
        "pages": null,
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Computer Science"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "title:longformer--the-long-document-transformer"
    },
    {
      "metadata": {
        "text": "David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. J. Mach. Learn. Res., 3(null):993-1022",
        "authors": [
          "Blei, D.",
          " Ng, A.",
          " Jordan, M."
        ],
        "affiliations": null,
        "title": "Latent dirichlet allocation",
        "abstract": null,
        "year": 2003,
        "journal": "J. Mach. Learn. Res.",
        "venue": "",
        "citation_count": 36763,
        "doi": null,
        "backup_id": null,
        "url": "https://www.semanticscholar.org/paper/4574d77fff19e093782178595a8988a7f3aa1969",
        "volume": "3",
        "issue": null,
        "pages": "993-1022",
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": null,
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "title:latent-dirichlet-allocation"
    },
    {
      "metadata": {
        "text": "Tianqi Chen, Tong He, Michael Benesty, Vadim Khotilovich, Yuan Tang, Hyunsu Cho, Kailong Chen, et al. 2015. XGBoost: Extreme Gradient Boosting. $R$ package version 0.4-2, 1(4):1-4",
        "authors": [
          "Chen, T.",
          "He, T.",
          "Benesty, M.",
          "Khotilovich, V.",
          "Tang, Y.",
          "Cho, H.",
          "Chen, K.",
          "et al."
        ],
        "affiliations": null,
        "title": "XGBoost: Extreme Gradient Boosting",
        "abstract": "Denial of Service (DoS) adalah salah satu serangan cyber populer yang ditargetkan pada situs web organisasi terkenal dan berpotensi memiliki biaya ekonomi dan waktu yang tinggi. Dalam makalah ini, beberapa metode pembelajaran mesin termasuk model ensemble dan pengklasifikasi deep learning berbasis autoencoder dibandingkan dan disetel menggunakan optimasi Bayesian. Kerangka autoencoder memungkinkan untuk mengekstrak fitur baru dengan memetakan input asli ke ruang baru. Metode tersebut dilatih dan diuji baik untuk klasifikasi biner dan multi-kelas pada kumpulan data Digiturk dan Labris, yang baru-baru ini diperkenalkan untuk mendeteksi berbagai jenis serangan DdoS. Semakin penting koneksi data melalui Internet membuat kebutuhan akan keamanan jaringan data semakin meningkat. Salah satu tools yang penting adalah Intrusion detection systems (IDS). Sistem Deteksi Intrusi (IDS) adalah proses pemantauan lalu lintas jaringan dalam sistem untuk mendeteksi pola dan aktivitas yang mencurigakan yang memungkinkan ada serangan dalam sistem itu. beberapa jenis serangan, yaitu Botnet, UDP, SYN, broadcast, sleep deprivation, dan serangan bertubi-tubi. klasifikasi pertama, hasilnya menunjukkan bahwa baik Precision (PR) dan Recall (RE) adalah 89% untuk Algoritma Random Forest. Akurasi rata-rata (AC) dari model yang kami usulkan adalah 89% yang luar biasa dan cukup baik. Pada klasifikasi kedua, hasilnya menunjukkan bahwa baik Precision (PR) dan Recall (RE)sekitar 90% untuk algoritma XGBoost. Akurasi rata-rata (AC) dari model yang kami sarankan adalah 90% pada dataset CICDDoS2019.",
        "year": 2015,
        "journal": "$R$ package version 0.4-2",
        "venue": "Jurnal CoSciTech (Computer Science and Information Technology)",
        "citation_count": 6,
        "doi": "10.37859/coscitech.v3i3.4356",
        "backup_id": null,
        "url": "https://www.semanticscholar.org/paper/09d0075e846f75e39074725b9870027f93b81997",
        "volume": "1",
        "issue": "4",
        "pages": "1-4",
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": null,
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "doi:10.37859/coscitech.v3i3.4356"
    },
    {
      "metadata": {
        "text": "Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416",
        "authors": [
          "Chung, H.W.",
          " Hou, L.",
          " Longpre, S.",
          " Zoph, B.",
          " Tay, Y.",
          " Fedus, W.",
          " Li, E.",
          " Wang, X.",
          " Dehghani, M.",
          " Brahma, S.",
          " et al."
        ],
        "affiliations": null,
        "title": "Scaling instruction-finetuned language models",
        "abstract": "Finetuning language models on a collection of datasets phrased as instructions has been shown to improve model performance and generalization to unseen tasks. In this paper we explore instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought data. We find that instruction finetuning with the above aspects dramatically improves performance on a variety of model classes (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and evaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation). For instance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PALM 540B by a large margin (+9.4% on average). Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints, which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.",
        "year": 2022,
        "journal": "arXiv preprint",
        "venue": "Journal of machine learning research",
        "citation_count": 2987,
        "doi": "10.48550/arXiv.2210.11416",
        "backup_id": "arXiv:2210.11416",
        "url": "https://www.semanticscholar.org/paper/cdbd4f9b6ab2e2fd1ddf5400d5ed2c18960635d1",
        "volume": null,
        "issue": null,
        "pages": null,
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Computer Science"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "doi:10.48550/arXiv.2210.11416"
    },
    {
      "metadata": {
        "text": "Leon Derczynski, Kalina Bontcheva, and Ian Roberts. 2016. Broad Twitter corpus: A diverse named entity recognition resource. In Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 11691179, Osaka, Japan. The COLING 2016 Organizing Committee.",
        "authors": [
          "Derczynski, L.",
          " Bontcheva, K.",
          " Roberts, I."
        ],
        "affiliations": null,
        "title": "Broad Twitter corpus: A diverse named entity recognition resource",
        "abstract": null,
        "year": 2016,
        "journal": "Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers",
        "venue": "International Conference on Computational Linguistics",
        "citation_count": 111,
        "doi": null,
        "backup_id": null,
        "url": "https://www.semanticscholar.org/paper/1863f4c3cb86737e0cc232a62e021548a4b48c05",
        "volume": null,
        "issue": null,
        "pages": "1169-1179",
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Computer Science"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "title:broad-twitter-corpus--a-diverse-named-entity-recognition-resource"
    },
    {
      "metadata": {
        "text": "Milan Dojchinovski, Dinesh Reddy, Tom\u00e1\u0161 Kliegr, Tom\u00e1\u0161 Vitvar, and Harald Sack. 2016. Crowdsourced corpus with entity salience annotations. In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16), pages 3307-3311, Portoro\u017e, Slovenia. European Language Resources Association (ELRA).",
        "authors": [
          "Dojchinovski, M.",
          " Reddy, D.",
          " Kliegr, T.",
          " Vitvar, T.",
          " Sack, H."
        ],
        "affiliations": null,
        "title": "Crowdsourced corpus with entity salience annotations",
        "abstract": null,
        "year": 2016,
        "journal": "Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16)",
        "venue": "International Conference on Language Resources and Evaluation",
        "citation_count": 17,
        "doi": null,
        "backup_id": null,
        "url": "https://www.semanticscholar.org/paper/428812309552c99b2fd32452f73846d078a2bac2",
        "volume": null,
        "issue": null,
        "pages": "3307-3311",
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Computer Science"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "title:crowdsourced-corpus-with-entity-salience-annotations"
    },
    {
      "metadata": {
        "text": "Jesse Dunietz and Daniel Gillick. 2014. A new entity salience task with millions of training examples. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, volume 2: Short Papers, pages 205-209, Gothenburg, Sweden. Association for Computational Linguistics.",
        "authors": [
          "Dunietz, J",
          " Gillick, D"
        ],
        "affiliations": null,
        "title": "A new entity salience task with millions of training examples",
        "abstract": "Although many NLP systems are moving toward entity-based processing, most still identify important phrases using classical keyword-based approaches. To bridge this gap, we introduce the task of entity salience: assigning a relevance score to each entity in a document. We demonstrate how a labeled corpus for the task can be automatically generated from a corpus of documents and accompanying abstracts. We then show how a classifier with features derived from a standard NLP pipeline outperforms a strong baseline by 34%. Finally, we outline initial experiments on further improving accuracy by leveraging background knowledge about the relationships between entities.",
        "year": 2014,
        "journal": "Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics",
        "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
        "citation_count": 70,
        "doi": "10.3115/v1/E14-4040",
        "backup_id": null,
        "url": "https://www.semanticscholar.org/paper/36a181c75ecf77be863cea34adb21398fadd7b5f",
        "volume": "2",
        "issue": "2",
        "pages": "205-209",
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Computer Science"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "doi:10.3115/v1/E14-4040"
    },
    {
      "metadata": {
        "text": "G\u00fcnes Erkan and Dragomir R. Radev. 2004. Lexrank: Graph-based lexical centrality as salience in text summarization. J. Artif. Int. Res., 22(1):457-479",
        "authors": [
          "Erkan, G.",
          " Radev, D.R."
        ],
        "affiliations": null,
        "title": "Lexrank: Graph-based lexical centrality as salience in text summarization",
        "abstract": "We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences. Our system, based on LexRank ranked in first place in more than one task in the recent DUC 2004 evaluation. In this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier DUC evaluations. We discuss several methods to compute centrality using the similarity graph. The results show that degree-based methods (including LexRank) outperform both centroid-based methods and other systems participating in DUC in most of the cases. Furthermore, the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank. We also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents.",
        "year": 2004,
        "journal": "J. Artif. Int. Res.",
        "venue": "Journal of Artificial Intelligence Research",
        "citation_count": 3074,
        "doi": "10.1613/jair.1523",
        "backup_id": null,
        "url": "https://www.semanticscholar.org/paper/44fca068eecce2203d111213e3691647914a3945",
        "volume": "22",
        "issue": "1",
        "pages": "457-479",
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Mathematics",
          "Computer Science"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "doi:10.1613/jair.1523"
    },
    {
      "metadata": {
        "text": "Michael Gamon, Tae Yano, Xinying Song, Johnson Apacible, and Patrick Pantel. 2013. Understanding document aboutness step one: Identifying salient entities. Technical Report MSR-TR-2013-73",
        "authors": [
          "Gamon, M",
          " Yano, T",
          " Song, X",
          " Apacible, J",
          " Pantel, P"
        ],
        "affiliations": null,
        "title": "Understanding document aboutness step one: Identifying salient entities",
        "abstract": null,
        "year": 2013,
        "journal": "Technical Report MSR-TR-2013-73",
        "venue": "",
        "citation_count": 8,
        "doi": null,
        "backup_id": null,
        "url": "https://www.semanticscholar.org/paper/0ce18c7d516d0dc61aabe10a552a47cde9c60b9d",
        "volume": null,
        "issue": null,
        "pages": null,
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Computer Science"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "title:understanding-document-aboutness-step-one--identifying-salient-entities"
    },
    {
      "metadata": {
        "text": "Pengcheng He, Jianfeng Gao, and Weizhu Chen. 2023. DeBERTav3: Improving deBERTa using ELECTRAstyle pre-training with gradient-disentangled embedding sharing. In The Eleventh International Conference on Learning Representations.",
        "authors": [
          "He, Pengcheng",
          "Gao, Jianfeng",
          "Chen, Weizhu"
        ],
        "affiliations": null,
        "title": "DeBERTav3: Improving deBERTa using ELECTRAstyle pre-training with gradient-disentangled embedding sharing",
        "abstract": null,
        "year": 2023,
        "journal": "The Eleventh International Conference on Learning Representations",
        "venue": null,
        "citation_count": null,
        "doi": null,
        "backup_id": null,
        "url": null,
        "volume": null,
        "issue": null,
        "pages": null,
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": null,
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "title:debertav3--improving-deberta-using-electrastyle-pre-training-with-gradient-disentangled-embedding-sharing"
    },
    {
      "metadata": {
        "text": "Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bordino, Hagen F\u00fcrstenau, Manfred Pinkal, Marc Spaniol, Bilyana Taneva, Stefan Thater, and Gerhard Weikum. 2011. Robust disambiguation of named entities in text. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 782-792, Edinburgh, Scotland, UK. Association for Computational Linguistics.",
        "authors": [
          "Hoffart, J",
          "Yosef, M A",
          "Bordino, I",
          "F\u00fcrstenau, H",
          "Pinkal, M",
          "Spaniol, M",
          "Taneva, B",
          "Thater, S",
          "Weikum, G"
        ],
        "affiliations": null,
        "title": "Robust disambiguation of named entities in text",
        "abstract": null,
        "year": 2011,
        "journal": "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citation_count": 1116,
        "doi": null,
        "backup_id": null,
        "url": "https://www.semanticscholar.org/paper/d95738f38d97a030d98508357e4d5c78a4a208ba",
        "volume": null,
        "issue": null,
        "pages": "782-792",
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Computer Science"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "title:robust-disambiguation-of-named-entities-in-text"
    },
    {
      "metadata": {
        "text": "Ella Hofmann-Coyle, Mayank Kulkarni, Lingjue Xie, Mounica Maddela, and Daniel Preotiuc-Pietro. 2022. Extractive entity-centric summarization as sentence selection using bi-encoders. In Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 326-333, Online only. Association for Computational Linguistics.",
        "authors": [
          "Hofmann-Coyle, E.",
          "Kulkarni, M.",
          "Xie, L.",
          "Maddela, M.",
          "Preotiuc-Pietro, D."
        ],
        "affiliations": null,
        "title": "Extractive entity-centric summarization as sentence selection using bi-encoders",
        "abstract": "Entity-centric summarization is a type of controllable summarization that aims to produce a summary of a document that is specific to a given target entity. Extractive summaries possess multiple advantages over abstractive ones such as preserving factuality and can be directly used in downstream tasks like target-based sentiment analysis or incorporated into search applications. In this paper, we explore methods to solve this task by recasting it as a sentence selection task, as supported by the EntSUM data set. We use methods inspired by information retrieval, where the input to the model is a pair representing a sentence from the original document and the target entity, in place of the query. We explore different architecture variants and loss functions in this framework with results showing an up to 5.8 F1 improvement over past state-of-the-art and outperforming the competitive entity-centric Lead 3 heuristic by 1.1 F1. In addition, we also demonstrate similarly strong results on the related task of salient sentence selection for an entity.",
        "year": 2022,
        "journal": "Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)",
        "venue": "AACL",
        "citation_count": 5,
        "doi": "10.18653/v1/2022.aacl-short.40",
        "backup_id": null,
        "url": "https://www.semanticscholar.org/paper/8ae5b8ea4caf89bdff356534186e5ff4f0fcebd2",
        "volume": null,
        "issue": "2",
        "pages": "326-333",
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Computer Science"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "doi:10.18653/v1/2022.aacl-short.40"
    },
    {
      "metadata": {
        "text": "Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. 2006. OntoNotes: The $90 % solution. In Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers, pages 57-60, New York City, USA. Association for Computational Linguistics.",
        "authors": [
          "Hovy, E.",
          " Marcus, M.",
          " Palmer, M.",
          " Ramshaw, L.",
          " Weischedel, R."
        ],
        "affiliations": null,
        "title": "OntoNotes: The $90 % solution",
        "abstract": "We describe the OntoNotes methodology and its result, a large multilingual richly-annotated corpus constructed at 90% interannotator agreement. An initial portion (300K words of English newswire and 250K words of Chinese newswire) will be made available to the community during 2007.",
        "year": 2006,
        "journal": "Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "citation_count": 1005,
        "doi": "10.3115/1614049.1614064",
        "backup_id": null,
        "url": "https://www.semanticscholar.org/paper/e54d8b07ef659f9ee2671441c4355e414e408836",
        "volume": null,
        "issue": null,
        "pages": "57-60",
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Computer Science"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "doi:10.3115/1614049.1614064"
    },
    {
      "metadata": {
        "text": "Xiaolei Huang and Michael J. Paul. 2018. Examining temporality in document classification. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 694-699, Melbourne, Australia. Association for Computational Linguistics.",
        "authors": [
          "Huang, X.",
          " Paul, M. J."
        ],
        "affiliations": null,
        "title": "Examining temporality in document classification",
        "abstract": "Many corpora span broad periods of time. Language processing models trained during one time period may not work well in future time periods, and the best model may depend on specific times of year (e.g., people might describe hotels differently in reviews during the winter versus the summer). This study investigates how document classifiers trained on documents from certain time intervals perform on documents from other time intervals, considering both seasonal intervals (intervals that repeat across years, e.g., winter) and non-seasonal intervals (e.g., specific years). We show experimentally that classification performance varies over time, and that performance can be improved by using a standard domain adaptation approach to adjust for changes in time.",
        "year": 2018,
        "journal": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citation_count": 41,
        "doi": "10.18653/v1/P18-2110",
        "backup_id": null,
        "url": "https://www.semanticscholar.org/paper/4f9f3699c0972bd93a2b81dd2d690f61a4c1495c",
        "volume": null,
        "issue": "2",
        "pages": "694-699",
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Computer Science"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "doi:10.18653/v1/P18-2110"
    },
    {
      "metadata": {
        "text": "Anette Hulth. 2003. Improved automatic keyword extraction given more linguistic knowledge. In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing, pages 216-223.",
        "authors": [
          "Hulth, A."
        ],
        "affiliations": null,
        "title": "Improved automatic keyword extraction given more linguistic knowledge",
        "abstract": "In this paper, experiments on automatic extraction of keywords from abstracts using a supervised machine learning algorithm are discussed. The main point of this paper is that by adding linguistic knowledge to the representation (such as syntactic features), rather than relying only on statistics (such as term frequency and n-grams), a better result is obtained as measured by keywords previously assigned by professional indexers. In more detail, extracting NP-chunks gives a better precision than n-grams, and by adding the PoS tag(s) assigned to the term as a feature, a dramatic improvement of the results is obtained, independent of the term selection approach applied.",
        "year": 2003,
        "journal": "Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citation_count": 1067,
        "doi": "10.3115/1119355.1119383",
        "backup_id": null,
        "url": "https://www.semanticscholar.org/paper/771ca13f78a6cfda9ed99004a386e9e7e187bd34",
        "volume": null,
        "issue": null,
        "pages": "216-223",
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Computer Science"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "doi:10.3115/1119355.1119383"
    },
    {
      "metadata": {
        "text": "Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu. 2017. Lightgbm: A highly efficient gradient boosting decision tree. Advances in neural information processing systems, 30.",
        "authors": [
          "Ke, G.",
          " Meng, Q.",
          " Finley, T.",
          " Wang, T.",
          " Chen, W.",
          " Ma, W.",
          " Ye, Q.",
          " Liu, T.-Y."
        ],
        "affiliations": null,
        "title": "Lightgbm: A highly efficient gradient boosting decision tree",
        "abstract": null,
        "year": 2017,
        "journal": "Advances in neural information processing systems",
        "venue": "Neural Information Processing Systems",
        "citation_count": 10185,
        "doi": null,
        "backup_id": null,
        "url": "https://www.semanticscholar.org/paper/497e4b08279d69513e4d2313a7fd9a55dfb73273",
        "volume": "30",
        "issue": null,
        "pages": null,
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Computer Science",
          "Mathematics"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "title:lightgbm--a-highly-efficient-gradient-boosting-decision-tree"
    },
    {
      "metadata": {
        "text": "Nikolaos Kolitsas, Octavian-Eugen Ganea, and Thomas Hofmann. 2018. End-to-end neural entity linking. In Proceedings of the 22nd Conference on Computational Natural Language Learning, pages 519-529, Brussels, Belgium. Association for Computational Linguistics.",
        "authors": [
          "Kolitsas, N.",
          " Ganea, O.-E.",
          " Hofmann, T."
        ],
        "affiliations": null,
        "title": "End-to-end neural entity linking",
        "abstract": "Entity Linking (EL) is an essential task for semantic text understanding and information extraction. Popular methods separately address the Mention Detection (MD) and Entity Disambiguation (ED) stages of EL, without leveraging their mutual dependency. We here propose the first neural end-to-end EL system that jointly discovers and links entities in a text document. The main idea is to consider all possible spans as potential mentions and learn contextual similarity scores over their entity candidates that are useful for both MD and ED decisions. Key components are context-aware mention embeddings, entity embeddings and a probabilistic mention - entity map, without demanding other engineered features. Empirically, we show that our end-to-end method significantly outperforms popular systems on the Gerbil platform when enough training data is available. Conversely, if testing datasets follow different annotation conventions compared to the training set (e.g. queries/ tweets vs news documents), our ED model coupled with a traditional NER system offers the best or second best EL accuracy.",
        "year": 2018,
        "journal": "Proceedings of the 22nd Conference on Computational Natural Language Learning",
        "venue": "Conference on Computational Natural Language Learning",
        "citation_count": 260,
        "doi": "10.18653/v1/K18-1050",
        "backup_id": null,
        "url": "https://www.semanticscholar.org/paper/f0462312d9e985f13fd20d65178f9565d967f07e",
        "volume": null,
        "issue": null,
        "pages": "519-529",
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Computer Science"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "doi:10.18653/v1/K18-1050"
    },
    {
      "metadata": {
        "text": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A robustly optimized BERT pretraining approach. arXiv preprint arXiv:1907.11692.",
        "authors": [
          "Liu, Y.",
          " Ott, M.",
          " Goyal, N.",
          " Du, J.",
          " Joshi, M.",
          " Chen, D.",
          " Levy, O.",
          " Lewis, M.",
          " Zettlemoyer, L.",
          " Stoyanov, V."
        ],
        "affiliations": null,
        "title": "RoBERTa: A robustly optimized BERT pretraining approach",
        "abstract": "Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.",
        "year": 2019,
        "journal": "arXiv preprint arXiv",
        "venue": "arXiv.org",
        "citation_count": 23856,
        "doi": null,
        "backup_id": "arXiv:1907.11692",
        "url": "https://www.semanticscholar.org/paper/077f8329a7b6fa3b7c877a57b81eb6c18b5f87de",
        "volume": null,
        "issue": null,
        "pages": null,
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Computer Science"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "arXiv:1907.11692"
    },
    {
      "metadata": {
        "text": "Ilya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization. In International Conference on Learning Representations.",
        "authors": [
          "Loshchilov, I.",
          " Hutter, F."
        ],
        "affiliations": null,
        "title": "Decoupled weight decay regularization",
        "abstract": null,
        "year": 2019,
        "journal": "International Conference on Learning Representations",
        "venue": "International Conference on Learning Representations",
        "citation_count": 21943,
        "doi": null,
        "backup_id": null,
        "url": "https://www.semanticscholar.org/paper/d07284a6811f1b2745d91bdb06b040b57f226882",
        "volume": null,
        "issue": null,
        "pages": null,
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Computer Science"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "title:decoupled-weight-decay-regularization"
    },
    {
      "metadata": {
        "text": "Mounica Maddela, Mayank Kulkarni, and Daniel Preotiuc-Pietro. 2022. EntSUM: A data set for entitycentric extractive summarization. In Proceedings of",
        "authors": [
          "Maddela, M",
          " Kulkarni, M",
          " Preotiuc-Pietro, D"
        ],
        "affiliations": null,
        "title": "EntSUM: A data set for entitycentric extractive summarization",
        "abstract": null,
        "year": 2022,
        "journal": "Proceedings of",
        "venue": null,
        "citation_count": null,
        "doi": null,
        "backup_id": null,
        "url": null,
        "volume": null,
        "issue": null,
        "pages": null,
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": null,
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "title:entsum--a-data-set-for-entitycentric-extractive-summarization"
    },
    {
      "metadata": {
        "text": "the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3355-3366, Dublin, Ireland. Association for Computational Linguistics",
        "authors": null,
        "affiliations": null,
        "title": null,
        "abstract": null,
        "year": null,
        "journal": "Association for Computational Linguistics",
        "venue": null,
        "citation_count": null,
        "doi": null,
        "backup_id": null,
        "url": null,
        "volume": "1",
        "issue": null,
        "pages": "3355-3366",
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": null,
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "title:the-60th-annual-meeting-of-the-association-for-computational-linguistics--volume-1--long-papers---pages-3355-3366--dublin--ireland--association-for-computational-linguistics"
    },
    {
      "metadata": {
        "text": "Edgar Meij, Wouter Weerkamp, and Maarten de Rijke. 2012. Adding semantics to microblog posts. In Proceedings of the Fifth ACM International Conference on Web Search and Data Mining, WSDM '12, page 563-572, New York, NY, USA. Association for Computing Machinery.",
        "authors": [
          "Meij, E.",
          "Weerkamp, W.",
          "de Rijke, M."
        ],
        "affiliations": null,
        "title": "Adding semantics to microblog posts",
        "abstract": null,
        "year": 2012,
        "journal": "Proceedings of the Fifth ACM International Conference on Web Search and Data Mining",
        "venue": "Web Search and Data Mining",
        "citation_count": 328,
        "doi": "10.1145/2124295.2124364",
        "backup_id": null,
        "url": "https://www.semanticscholar.org/paper/7801fdb0aa7db9301c7363f7cd68b43f97adafef",
        "volume": null,
        "issue": null,
        "pages": "563-572",
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Computer Science"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "doi:10.1145/2124295.2124364"
    },
    {
      "metadata": {
        "text": "Rada Mihalcea and Paul Tarau. 2004. TextRank: Bringing order into text. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 404-411, Barcelona, Spain. Association for Computational Linguistics.",
        "authors": [
          "Mihalcea, R.",
          " Tarau, P."
        ],
        "affiliations": null,
        "title": "TextRank: Bringing order into text",
        "abstract": null,
        "year": 2004,
        "journal": "Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citation_count": 3945,
        "doi": null,
        "backup_id": null,
        "url": "https://www.semanticscholar.org/paper/7b95d389bc6affe6a127d53b04bcfd68138f1a1a",
        "volume": null,
        "issue": null,
        "pages": "404-411",
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Computer Science"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "title:textrank--bringing-order-into-text"
    },
    {
      "metadata": {
        "text": "Dat Ba Nguyen, Johannes Hoffart, Martin Theobald, and Gerhard Weikum. 2014. Aida-light: High-throughput named-entity disambiguation. In Proceedings of the Workshop on Linked Data on the Web co-located with the 23rd International World Wide Web Conference (WWW 2014), Seoul, Korea, April 8, 2014, volume 1184 of CEUR Workshop Proceedings. CEURWS.org.\nF. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:28252830.",
        "authors": [
          "Nguyen, D.B.",
          " Hoffart, J.",
          " Theobald, M.",
          " Weikum, G."
        ],
        "affiliations": null,
        "title": "Aida-light: High-throughput named-entity disambiguation",
        "abstract": null,
        "year": 2014,
        "journal": "CEUR Workshop Proceedings",
        "venue": "LDOW",
        "citation_count": 75,
        "doi": null,
        "backup_id": null,
        "url": "https://www.semanticscholar.org/paper/fbe0103f2754c9a7ccb3557e67540aebf926ea35",
        "volume": null,
        "issue": "1184",
        "pages": null,
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Computer Science"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "title:aida-light--high-throughput-named-entity-disambiguation"
    },
    {
      "metadata": {
        "text": "Marco Ponza, Diego Ceccarelli, Paolo Ferragina, Edgar Meij, and Sambhav Kothari. 2021. Contextualizing trending entities in news stories. In Proceedings of the 14th ACM International Conference on Web Search and Data Mining, pages 346-354.",
        "authors": [
          "Ponzam, M.",
          " Ceccarelli, D.",
          " Ferragina, P.",
          " Meij, E.",
          " Kothari, S."
        ],
        "affiliations": null,
        "title": "Contextualizing trending entities in news stories",
        "abstract": "Trends are those keywords, phrases, or names that are mentioned most often on social media or in news in a particular timeframe.They are an effective way for human news readers to both discover and stay focused on the most relevant information of the day. In this work, we consider trends that correspond to an entity in a knowledge graph and introduce the new and as-yet unexplored task of identifying other entities that may help explain the \"why\" an entity is trending. We refer to these retrieved entities as contextual entities. Some of them are more important than others in the context of the trending entity and we thus determine a ranking of entities according to how useful they are in contextualizing the trend.We propose two solutions for ranking contextual entities. The first one is fully unsupervised and based on Personalized PageRank, calculated over a trending entity-specific graph of other entities where the edges encode a notion of directional similarity based on embedded background knowledge. Our second method is based on learning to rank and combines the intuitions behind the unsupervised model with signals derived from hand-crafted features in a supervised setting. We compare our models on this novel task by using a new, purpose-built test collection created using crowdsourcing. Our methods improve over the strongest baseline in terms ofPrecision at 1 by 7% (unsupervised) and 13% (supervised). We find that the salience of a contextual entity and how coherent it is with respect to the news story are strong indicators of relevance in both unsupervised and supervised settings.",
        "year": 2021,
        "journal": "Proceedings of the 14th ACM International Conference on Web Search and Data Mining",
        "venue": "Web Search and Data Mining",
        "citation_count": 6,
        "doi": "10.1145/3437963.3441765",
        "backup_id": null,
        "url": "https://www.semanticscholar.org/paper/59a5379f3a6ae64f7a36e3d8762d736791cd754a",
        "volume": null,
        "issue": null,
        "pages": "346-354",
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Computer Science"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "doi:10.1145/3437963.3441765"
    },
    {
      "metadata": {
        "text": "Marco Ponza, Paolo Ferragina, and Francesco Piccinno. 2019. Swat: A system for detecting salient wikipedia entities in texts. Computational Intelligence, 35(4):858-890.",
        "authors": [
          "Panza, M.",
          "Ferragina, P.",
          "Piccinno, F."
        ],
        "affiliations": null,
        "title": "Swat: A system for detecting salient wikipedia entities in texts",
        "abstract": "We study the problem of entity salience by proposing the design and implementation of Swat, a system that identifies the salient Wikipedia entities occurring in an input document. Swat consists of several modules that are able to detect and classify on\u2010the\u2010fly Wikipedia entities as salient or not, based on a large number of syntactic, semantic, and latent features properly extracted via a supervised process, which has been trained over millions of examples drawn from the New York Times corpus. The validation process is performed through a large experimental assessment, eventually showing that Swat improves known solutions over all publicly available datasets. We release Swat via an API that we describe and comment in the paper to ease its use in other software.",
        "year": 2019,
        "journal": "Computational Intelligence",
        "venue": "International Conference on Climate Informatics",
        "citation_count": 19,
        "doi": "10.1111/coin.12216",
        "backup_id": null,
        "url": "https://www.semanticscholar.org/paper/3c6eade9fe18aa1dd40b77b94ef2824cc700ded7",
        "volume": "35",
        "issue": "4",
        "pages": "858-890",
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Computer Science"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "doi:10.1111/coin.12216"
    },
    {
      "metadata": {
        "text": "Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, Hwee Tou Ng, Anders Bj\u00f6rkelund, Olga Uryupina, Yuchen Zhang, and Zhi Zhong. 2013. Towards robust linguistic analysis using OntoNotes. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 143-152, Sofia, Bulgaria. Association for Computational Linguistics.",
        "authors": [
          "Pradhan, S.",
          "Moschitti, A.",
          " Xue, N.",
          " Ng, H. T.",
          " Bj\u00f6rkelund, A.",
          " Uryupina, O.",
          " Zhang, Y.",
          " Zhong, Z."
        ],
        "affiliations": null,
        "title": "Towards robust linguistic analysis using OntoNotes",
        "abstract": null,
        "year": 2013,
        "journal": "Proceedings of the Seventeenth Conference on Computational Natural Language Learning",
        "venue": "Conference on Computational Natural Language Learning",
        "citation_count": 456,
        "doi": null,
        "backup_id": null,
        "url": "https://www.semanticscholar.org/paper/c92970286c535992a86539b761357761e97a37ee",
        "volume": null,
        "issue": null,
        "pages": "143-152",
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Computer Science"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "title:towards-robust-linguistic-analysis-using-ontonotes"
    },
    {
      "metadata": {
        "text": "Shruti Rijhwani and Daniel Preotiuc-Pietro. 2020. Temporally-informed analysis of named entity recognition. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7605-7617, Online. Association for Computational Linguistics.",
        "authors": [
          "Rijhwani, S",
          " Preotiuc-Pietro, D"
        ],
        "affiliations": null,
        "title": "Temporally-informed analysis of named entity recognition",
        "abstract": "Natural language processing models often have to make predictions on text data that evolves over time as a result of changes in language use or the information described in the text. However, evaluation results on existing data sets are seldom reported by taking the timestamp of the document into account. We analyze and propose methods that make better use of temporally-diverse training data, with a focus on the task of named entity recognition. To support these experiments, we introduce a novel data set of English tweets annotated with named entities. We empirically demonstrate the effect of temporal drift on performance, and how the temporal information of documents can be used to obtain better models compared to those that disregard temporal information. Our analysis gives insights into why this information is useful, in the hope of informing potential avenues of improvement for named entity recognition as well as other NLP tasks under similar experimental setups.",
        "year": 2020,
        "journal": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citation_count": 56,
        "doi": "10.18653/v1/2020.acl-main.680",
        "backup_id": null,
        "url": "https://www.semanticscholar.org/paper/509b42fc150a057a64c4608f64e779ef04fdff47",
        "volume": null,
        "issue": null,
        "pages": "7605-7617",
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Computer Science"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "doi:10.18653/v1/2020.acl-main.680"
    },
    {
      "metadata": {
        "text": "Evan Sandhaus. 2008. The new york times annotated corpus. Linguistic Data Consortium, Philadelphia, 6(12):e26752",
        "authors": [
          "Sandhaus, E."
        ],
        "affiliations": null,
        "title": "The new york times annotated corpus",
        "abstract": null,
        "year": 2008,
        "journal": "Linguistic Data Consortium",
        "venue": "International Conference on Knowledge Engineering and the Semantic Web",
        "citation_count": 11,
        "doi": "10.1007/978-3-642-41360-5_7",
        "backup_id": null,
        "url": "https://www.semanticscholar.org/paper/4adb79a91a617e7b13bc3eaa4478c7dd6df73568",
        "volume": "6",
        "issue": "12",
        "pages": "e26752",
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Computer Science"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "doi:10.1007/978-3-642-41360-5_7"
    },
    {
      "metadata": {
        "text": "Benjamin Strauss, Bethany Toma, Alan Ritter, MarieCatherine De Marneffe, and Wei Xu. 2016. Results of the w-nut 2016 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), pages 138-144.",
        "authors": [
          "Strauss, B.",
          " Toma, B.",
          " Ritter, A.",
          " De Marneffe, M.",
          " Xu, W."
        ],
        "affiliations": null,
        "title": "Results of the w-nut 2016 named entity recognition shared task",
        "abstract": null,
        "year": 2016,
        "journal": "Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT)",
        "venue": "NUT@COLING",
        "citation_count": 134,
        "doi": null,
        "backup_id": null,
        "url": "https://www.semanticscholar.org/paper/28ac5c728449c19be229e8839f5b5d6acd896f15",
        "volume": null,
        "issue": null,
        "pages": "138-144",
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Computer Science"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "title:results-of-the-w-nut-2016-named-entity-recognition-shared-task"
    },
    {
      "metadata": {
        "text": "Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung, Siamak Shakeri, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Denny Zhou, Neil Houlsby, and Donald Metzler. 2023. UI2: Unifying language learning paradigms.",
        "authors": [
          "Tay, Yi",
          " Dehghani, Mostafa",
          " Tran, Vinh Q.",
          " Garcia, Xavier",
          " Wei, Jason",
          " Wang, Xuezhi",
          " Chung, Hyung Won",
          " Shakeri, Siamak",
          " Bahri, Dara",
          " Schuster, Tal",
          " Zheng, Huaixiu Steven",
          " Zhou, Denny",
          " Houlsby, Neil",
          " Metzler, Donald"
        ],
        "affiliations": null,
        "title": "UI2: Unifying language learning paradigms",
        "abstract": "Existing pre-trained models are generally geared towards a particular class of problems. To date, there seems to be still no consensus on what the right architecture and pre-training setup should be. This paper presents a unified framework for pre-training models that are universally effective across datasets and setups. We begin by disentangling architectural archetypes with pre-training objectives -- two concepts that are commonly conflated. Next, we present a generalized&unified perspective for self-supervision in NLP and show how different pre-training objectives can be cast as one another and how interpolating between different objectives can be effective. We then propose Mixture-of-Denoisers (MoD), a pre-training objective that combines diverse pre-training paradigms together. We furthermore introduce a notion of mode switching, wherein downstream fine-tuning is associated with specific pre-training schemes. We conduct extensive ablative experiments to compare multiple pre-training objectives and find that our method pushes the Pareto-frontier by outperforming T5&GPT-like models across multiple diverse setups. By scaling our model up to 20B parameters, we achieve SOTA performance on 50 well-established supervised finetuning based NLP tasks. Our model also achieve strong results at in-context learning, outperforming 175B GPT-3 on zero-shot SuperGLUE and tripling the performance of T5-XXL on one-shot summarization. On 0-shot MMLU, UL2 20B outperforms T0 and T5 models. UL2 20B also works well with chain-of-thought prompting and reasoning, making it an appealing choice for research into reasoning at a small to medium scale of 20B parameters. Finally, we apply FLAN instruction tuning to the UL2 20B model, achieving MMLU and Big-Bench scores competitive to FLAN-PaLM 62B. We release Flax-based T5X checkpoints for the UL2 20B&Flan-UL2 20B.",
        "year": 2023,
        "journal": null,
        "venue": null,
        "citation_count": null,
        "doi": null,
        "backup_id": null,
        "url": null,
        "volume": null,
        "issue": null,
        "pages": null,
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": null,
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "title:ui2--unifying-language-learning-paradigms"
    },
    {
      "metadata": {
        "text": "Ian Tenney, Dipanjan Das, and Ellie Pavlick. 2019. BERT rediscovers the classical NLP pipeline. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 45934601, Florence, Italy. Association for Computational Linguistics.",
        "authors": [
          "Tenney, I.",
          " Das, D.",
          " Pavlick, E."
        ],
        "affiliations": null,
        "title": "BERT rediscovers the classical NLP pipeline",
        "abstract": "Pre-trained text encoders have rapidly advanced the state of the art on many NLP tasks. We focus on one such model, BERT, and aim to quantify where linguistic information is captured within the network. We find that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. Qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lower-level decisions on the basis of disambiguating information from higher-level representations.",
        "year": 2019,
        "journal": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citation_count": 1435,
        "doi": "10.18653/v1/P19-1452",
        "backup_id": null,
        "url": "https://www.semanticscholar.org/paper/97906df07855b029b7aae7c2a1c6c5e8df1d531c",
        "volume": null,
        "issue": null,
        "pages": "4593-4601",
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Computer Science"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "doi:10.18653/v1/P19-1452"
    },
    {
      "metadata": {
        "text": "Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R Thomas McCoy, Najoung Kim, Benjamin Van Durme, Samuel R Bowman, Dipanjan Das, et al. 2018. What do you learn from context? probing for sentence structure in contextualized word representations. In International Conference on Learning Representations.",
        "authors": [
          "Tenney, I.",
          "Xia, P.",
          "Chen, B.",
          "Wang, A.",
          "Poliak, A.",
          "McCoy, R T.",
          "Kim, N.",
          "Van Durme, B.",
          "Bowman, S R",
          "Das, D",
          "et al."
        ],
        "affiliations": null,
        "title": "What do you learn from context? probing for sentence structure in contextualized word representations",
        "abstract": "Contextualized representation models such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018) have recently achieved state-of-the-art results on a diverse array of downstream NLP tasks. Building on recent token-level probing work, we introduce a novel edge probing task design and construct a broad suite of sub-sentence tasks derived from the traditional structured NLP pipeline. We probe word-level contextual representations from four recent models and investigate how they encode sentence structure across a range of syntactic, semantic, local, and long-range phenomena. We find that existing models trained on language modeling and translation produce strong representations for syntactic phenomena, but only offer comparably small improvements on semantic tasks over a non-contextual baseline.",
        "year": 2018,
        "journal": "International Conference on Learning Representations",
        "venue": "International Conference on Learning Representations",
        "citation_count": 848,
        "doi": null,
        "backup_id": null,
        "url": "https://www.semanticscholar.org/paper/e2587eddd57bc4ba286d91b27c185083f16f40ee",
        "volume": null,
        "issue": null,
        "pages": null,
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Computer Science"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "title:what-do-you-learn-from-context--probing-for-sentence-structure-in-contextualized-word-representations"
    },
    {
      "metadata": {
        "text": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open foundation and finetuned chat models.",
        "authors": [
          "Touvron, H.",
          "Martin, L.",
          "Stone, K.",
          "Albert, P.",
          "Almahairi, A.",
          "Babaei, Y.",
          "Bashlykov, N.",
          "Batra, S.",
          "Bhargava, P.",
          "Bhosale, S.",
          "Bikel, D.",
          "Blecher, L.",
          "Canton Ferrer, C.",
          "Chen, M.",
          "Cucurull, G.",
          "Esiobu, D.",
          "Fernandes, J.",
          "Fu, J.",
          "Fu, W.",
          "Fuller, B.",
          "Gao, C.",
          "Goswami, V.",
          "Goyal, N.",
          "Hartshorn, A.",
          "Hosseini, S.",
          "Hou, R.",
          "Inan, H.",
          "Kardas, M.",
          "Kerkez, V.",
          "Khabsa, M.",
          "Kloumann, I.",
          "Korenev, A.",
          "Koura, P.",
          "Lachaux, M.",
          "Lavril, T.",
          "Lee, J.",
          "Liskovich, D.",
          "Lu, Y.",
          "Mao, Y.",
          "Martine, X.",
          "Mihaylov, T.",
          "Mishra, P.",
          "Molybog, I.",
          "Nie, Y.",
          "Poulton, A.",
          "Reizenstein, J.",
          "Rungta, R.",
          "Saladi, K.",
          "Schelten, A.",
          "Silva, R.",
          "Smith, E.",
          "Subramanian, R.",
          "Tan, X.",
          "Tang, B.",
          "Taylor, R.",
          "Williams, A.",
          "Xiang Kuan, J.",
          "Xu, P.",
          "Yan, Z.",
          "Zarov, I.",
          "Zhang, Y.",
          "Fan, A.",
          "Kambadur, M.",
          "Narang, S.",
          "Rodriguez, A.",
          "Stojnic, R.",
          "Edunov, S.",
          "Scialom, T."
        ],
        "affiliations": null,
        "title": "Llama 2: Open foundation and finetuned chat models",
        "abstract": "In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.",
        "year": 2023,
        "journal": "ArXiv",
        "venue": "arXiv.org",
        "citation_count": 10982,
        "doi": null,
        "backup_id": null,
        "url": "https://www.semanticscholar.org/paper/104b0bb1da562d53cbda87aec79ef6a2827d191a",
        "volume": null,
        "issue": null,
        "pages": null,
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Computer Science"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "title:llama-2--open-foundation-and-finetuned-chat-models"
    },
    {
      "metadata": {
        "text": "Salvatore Trani, Claudio Lucchese, Raffaele Perego, David E. Losada, Diego Ceccarelli, and Salvatore Orlando. 2018. Sel: A unified algorithm for salient entity linking. Computational Intelligence, 34(1):229.",
        "authors": [
          "Trani, S.",
          "Lucchese, C.",
          "Perego, R.",
          "Losada, D.E.",
          "Ceccarelli, D.",
          "Orlando, S."
        ],
        "affiliations": null,
        "title": "Sel: A unified algorithm for salient entity linking",
        "abstract": null,
        "year": 2018,
        "journal": "Computational Intelligence",
        "venue": "International Conference on Climate Informatics",
        "citation_count": 15,
        "doi": "10.1111/coin.12147",
        "backup_id": null,
        "url": "https://www.semanticscholar.org/paper/08b1182003c682409d1c53b70dc9562f9c430754",
        "volume": "34",
        "issue": "1",
        "pages": "229",
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Computer Science"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "doi:10.1111/coin.12147"
    },
    {
      "metadata": {
        "text": "Chuan Wu, Evangelos Kanoulas, Maarten de Rijke, and Wei Lu. 2020a. Wn-salience: A corpus of news articles with entity salience annotations. In Proceedings of The 12th Language Resources and Evaluation Conference, pages 2095-2102.",
        "authors": [
          "Wu, C.",
          " Kanoulas, E.",
          " de Rijke, M.",
          " Lu, W."
        ],
        "affiliations": null,
        "title": "Wn-salience: A corpus of news articles with entity salience annotations",
        "abstract": null,
        "year": 2020,
        "journal": "Proceedings of The 12th Language Resources and Evaluation Conference",
        "venue": "International Conference on Language Resources and Evaluation",
        "citation_count": 10,
        "doi": null,
        "backup_id": null,
        "url": "https://www.semanticscholar.org/paper/36d652a066dbe83794bd201e25519d067d43c816",
        "volume": null,
        "issue": null,
        "pages": "2095-2102",
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Computer Science"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "title:wn-salience--a-corpus-of-news-articles-with-entity-salience-annotations"
    },
    {
      "metadata": {
        "text": "Chuan Wu, Evangelos Kanoulas, and Maarten de Rijke. 2020b. Learning entity-centric document representations using an entity facet topic model. Inf. Process. Manage., 57(3).",
        "authors": [
          "Wu, C.",
          "Kanoulas, E.",
          "de Rijke, M."
        ],
        "affiliations": null,
        "title": "Learning entity-centric document representations using an entity facet topic model",
        "abstract": null,
        "year": 2020,
        "journal": "Inf. Process. Manage.",
        "venue": "Information Processing & Management",
        "citation_count": 9,
        "doi": "10.1016/j.ipm.2020.102216",
        "backup_id": null,
        "url": "https://www.semanticscholar.org/paper/92f7ddd8c70758aea6572942b72f1a9d9fb38e3e",
        "volume": "57",
        "issue": "3",
        "pages": null,
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Computer Science"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "doi:10.1016/j.ipm.2020.102216"
    },
    {
      "metadata": {
        "text": "Chenyan Xiong, Zhengzhong Liu, Jamie Callan, and Tie-Yan Liu. 2018. Towards better text understanding and retrieval through kernel entity salience modeling. In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, pages 575-584.",
        "authors": [
          "Xiong, C.",
          " Liu, Z.",
          " Callan, J.",
          " Liu, T.-Y."
        ],
        "affiliations": null,
        "title": "Towards better text understanding and retrieval through kernel entity salience modeling",
        "abstract": "This paper presents a Kernel Entity Salience Model (KESM) that improves text understanding and retrieval by better estimating entity salience (importance) in documents. KESM represents entities by knowledge enriched distributed representations, models the interactions between entities and words by kernels, and combines the kernel scores to estimate entity salience. The whole model is learned end-to-end using entity salience labels. The salience model also improves ad hoc search accuracy, providing effective ranking features by modeling the salience of query entities in candidate documents. Our experiments on two entity salience corpora and two TREC ad hoc search datasets demonstrate the effectiveness of KESM over frequency-based and feature-based methods. We also provide examples showing how KESM conveys its text understanding ability learned from entity salience to search.",
        "year": 2018,
        "journal": "The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval",
        "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
        "citation_count": 43,
        "doi": "10.1145/3209978.3209982",
        "backup_id": null,
        "url": "https://www.semanticscholar.org/paper/dfe44a8efb1ae6021a52abf4f75faeb447923377",
        "volume": null,
        "issue": null,
        "pages": "575-584",
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Computer Science"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "doi:10.1145/3209978.3209982"
    },
    {
      "metadata": {
        "text": "Lingyun Zhao, Lin Li, Xinhao Zheng, and Jianwei Zhang. 2021. A bert based sentiment analysis and key entity detection approach for online financial texts. In 2021 IEEE 24th International Conference on Computer Supported Cooperative Work in Design (CSCWD), pages 1233-1238. IEEE.",
        "authors": [
          "Zhao, L.",
          " Li, L.",
          " Zheng, X.",
          " Zhang, J."
        ],
        "affiliations": null,
        "title": "A bert based sentiment analysis and key entity detection approach for online financial texts",
        "abstract": "The emergence and rapid progress of the Internet have brought ever-increasing impact on financial domain. How to rapidly and accurately mine the key information from the massive negative financial texts has become one of the key issues for investors and decision Rakers. Aiming at the issue, we propose a sentiment analysis and key entity detection approach based on BERT, which is applied in online financial text mining and public opinion analysis in social media. By using pre-train model, we first study sentiment analysis, and then we consider key entity detection as a sentence matching or Machine Reading Comprehension (MRC) task in different granularity. Among them, we mainly focus on negative sentimental information. We detect the specific entity by using our approach, which is different from traditional Named Entity Recognition (NER). In addition, we also use ensemble learning to improve the performance of proposed approach. Experimental results show that the performance of our approach is generally higher than SVM, LR, NBM, and BERT for two financial sentiment analysis and key entity detection datasets.",
        "year": 2021,
        "journal": "2021 IEEE 24th International Conference on Computer Supported Cooperative Work in Design (CSCWD)",
        "venue": "International Conference on Computer Supported Cooperative Work in Design",
        "citation_count": 66,
        "doi": "10.1109/CSCWD49262.2021.9437616",
        "backup_id": null,
        "url": "https://www.semanticscholar.org/paper/5dcf6e5169c33bcdc7cede09c3e1976253bfd6f3",
        "volume": null,
        "issue": null,
        "pages": "1233-1238",
        "is_document_citation": false,
        "formatted": null,
        "fields_of_study": [
          "Computer Science"
        ],
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "doi:10.1109/CSCWD49262.2021.9437616"
    },
    {
      "metadata": {
        "text": "# Distillation of encoder-decoder transformers for sequence labellinging \n\nMarco Farina ${ }^{a 1}$ Duccio Pappadopulo ${ }^{a 1}$ Anant Gupta ${ }^{1}$ Leslie Huang ${ }^{1}$ Ozan \u0130rsoy ${ }^{1}$ Thamar Solorio ${ }^{11,2}$ { }^{1} Bloomberg { }^{2} Department of Computer Science, University of Houston {mfarina19, dpappadopulo, aqupta968, lhuang328, oirsoy}@bloomberg.net tsolorio@uh.edu \n\n#### Abstract\n\nDriven by encouraging results on a wide range of tasks, the field of NLP is experiencing an accelerated race to develop bigger language models. This race for bigger models has also underscored the need to continue the pursuit of practical distillation approaches that can leverage the knowledge acquired by these big models in a compute-efficient manner. Having this goal in mind, we build on recent work to propose a hallucination-free framework for sequence tagging that is especially suited for distillation. We show empirical results of new state-of-the-art performa",
        "authors": [
          "Farina, Marco",
          "Pappadopulo, Duccio",
          "Gupta, Anant",
          "Huang, Leslie",
          "\u0130rsoy, Ozan",
          "Solorio, Thamar"
        ],
        "affiliations": [
          "Bloomberg",
          "Department of Computer Science, University of Houston"
        ],
        "title": "Distillation of encoder-decoder transformers for sequence labellinging",
        "abstract": null,
        "year": null,
        "journal": null,
        "venue": null,
        "citation_count": null,
        "doi": null,
        "backup_id": null,
        "url": null,
        "volume": null,
        "issue": null,
        "pages": null,
        "is_document_citation": true,
        "formatted": null,
        "fields_of_study": null,
        "reference_count": null,
        "influential_citation_count": null,
        "is_open_access": null,
        "s2_fields_of_study": null
      },
      "id": "title:distillation-of-encoder-decoder-transformers-for-sequence-labellinging"
    }
  ],
  "links": [
    {
      "data": {
        "citation_text": "Distillation of encoder-decoder transformers for sequence labellingMarco Farina  Duccio Pappadopulo Anant GuptaLeslie Huang Ozan  I rsoy Thamar Solorio Bloomberg Department of Computer Science, University of Houston{mfarina19, dpappadopulo, aqupta968, lhuang328, oirsoy}@bloomberg.nettsolorio@uh.edu#### AbstractDriven by encouraging results on a wide range of tasks, the field of NLP is experiencing an accelerated race to develop bigger language models. This race for bigger models has also underscored the need to continue the pursuit of practical distillation approaches that can leverage the knowledge acquired by these big models in a compute-efficient manner. Having this goal in mind, we build on recent work to propose a hallucination-free framework for sequence tagging that is especially suited for distillation. We show empirical results of new state-of-the-art performa"
      },
      "source": "doi:10.48550/arXiv.2302.05454",
      "target": "doi:10.48550/arXiv.2302.05454"
    },
    {
      "data": {
        "citation_text": "SLS corpora. https://groups.csail.mit.edu/sls/downloads/. Accessed: 2022-09-09."
      },
      "source": "doi:10.48550/arXiv.2302.05454",
      "target": "doi:10.21437/ICSLP.1992-277"
    },
    {
      "data": {
        "citation_text": "Mikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, Sam Shleifer, Xi Victoria Lin, Jingfei Du, Srinivasan Iyer, Ramakanth Pasunuru, Giri Anantharaman, Xian Li, Shuohui Chen, Halil Akin, Mandeep Baines, Louis Martin, Xing Zhou, Punit Singh Koura, Brian O'Horo, Jeff Wang, Luke Zettlemoyer, Mona Diab, Zornitsa Kozareva, and Ves Stoyanov. 2021. Efficient large scale language modeling with mixtures of experts."
      },
      "source": "doi:10.48550/arXiv.2302.05454",
      "target": "doi:10.18653/v1/2022.emnlp-main.804"
    },
    {
      "data": {
        "citation_text": "Ben Athiwaratkun, Cicero Nogueira dos Santos, Jason Krone, and Bing Xiang. 2020. Augmented natural language for generative sequence labeling. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 375-385, Online. Association for Computational Linguistics."
      },
      "source": "doi:10.48550/arXiv.2302.05454",
      "target": "doi:10.18653/v1/2020.emnlp-main.27"
    },
    {
      "data": {
        "citation_text": "Sid Black, Gao Leo, Phil Wang, Connor Leahy, and Stella Biderman. 2021. GPT-Neo: Large Scale Autoregressive Language Modeling with MeshTensorflow. If you use this software, please cite it using these metadata."
      },
      "source": "doi:10.48550/arXiv.2302.05454",
      "target": "doi:10.5281/ZENODO.5297715"
    },
    {
      "data": {
        "citation_text": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877-1901. Curran Associates, Inc."
      },
      "source": "doi:10.48550/arXiv.2302.05454",
      "target": "title:language-models-are-few-shot-learners"
    },
    {
      "data": {
        "citation_text": "Cristian Bucilua, Rich Caruana, and Alexandru Niculescu-Mizil. 2006. Model compression. In Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '06, page 535-541, New York, NY, USA. Association for Computing Machinery."
      },
      "source": "doi:10.48550/arXiv.2302.05454",
      "target": "doi:10.48550/arXiv.2403.07378"
    },
    {
      "data": {
        "citation_text": "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311"
      },
      "source": "doi:10.48550/arXiv.2302.05454",
      "target": "arXiv:2204.02311"
    },
    {
      "data": {
        "citation_text": "Alice Coucke, Alaa Saade, Adrien Ball, Th\u00e9odore Bluche, Alexandre Caulier, David Leroy, Cl\u00e9ment Doumouro, Thibault Gisselbrecht, Francesco Caltagirone, Thibaut Lavril, Ma\u00ebl Primet, and Joseph Dureau. 2018. Snips voice platform: an embedded spoken language understanding system for private-by-design voice interfaces."
      },
      "source": "doi:10.48550/arXiv.2302.05454",
      "target": "title:snips-voice-platform--an-embedded-spoken-language-understanding-system-for-private-by-design-voice-interfaces"
    },
    {
      "data": {
        "citation_text": "Nicola De Cao, Gautier Izacard, Sebastian Riedel, and Fabio Petroni. 2020. Autoregressive entity retrieval."
      },
      "source": "doi:10.48550/arXiv.2302.05454",
      "target": "title:autoregressive-entity-retrieval"
    },
    {
      "data": {
        "citation_text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics."
      },
      "source": "doi:10.48550/arXiv.2302.05454",
      "target": "doi:10.18653/v1/N19-1423"
    },
    {
      "data": {
        "citation_text": "Tommaso Furlanello, Zachary C. Lipton, Michael Tschannen, Laurent Itti, and Anima Anandkumar. 2018. Born again neural networks."
      },
      "source": "doi:10.48550/arXiv.2302.05454",
      "target": "title:born-again-neural-networks"
    },
    {
      "data": {
        "citation_text": "Chih-Wen Goo, Guang Gao, Yun-Kai Hsu, Chih-Li Hao, Tsung-Chieh Chen, Keng-Wei Hsu, and Yun-Nung Chen. 2018. Slot-gated modeling for joint slot filling and intent prediction. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 753-757, New Orleans, Louisiana. Association for Computational Linguistics."
      },
      "source": "doi:10.48550/arXiv.2302.05454",
      "target": "doi:10.18653/v1/N18-2118"
    },
    {
      "data": {
        "citation_text": "Charles T. Hemphill, John J. Godfrey, and George R. Doddington. 1990. The ATIS spoken language systems pilot corpus. In Speech and Natural Language: Proceedings of a Workshop Held at Hidden Valley, Pennsylvania, June 24-27,1990."
      },
      "source": "doi:10.48550/arXiv.2302.05454",
      "target": "doi:10.3115/116580.116613"
    },
    {
      "data": {
        "citation_text": "Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in a neural network."
      },
      "source": "doi:10.48550/arXiv.2302.05454",
      "target": "title:distilling-the-knowledge-in-a-neural-network"
    },
    {
      "data": {
        "citation_text": "Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long short-term memory."
      },
      "source": "doi:10.48550/arXiv.2302.05454",
      "target": "doi:10.1162/neco.1997.9.8.1735"
    },
    {
      "data": {
        "citation_text": "Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019. Parameter-efficient transfer learning for NLP. In Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 2790-2799. PMLR."
      },
      "source": "doi:10.48550/arXiv.2302.05454",
      "target": "title:parameter-efficient-transfer-learning-for-nlp"
    },
    {
      "data": {
        "citation_text": "Huggingface. Models - Hugging Face.\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. 2020. TinyBERT: Distilling BERT for natural language understanding. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 41634174, Online. Association for Computational Linguistics."
      },
      "source": "doi:10.48550/arXiv.2302.05454",
      "target": "doi:10.18653/v1/2020.findings-emnlp.372"
    },
    {
      "data": {
        "citation_text": "Haoran Li, Abhinav Arora, Shuohui Chen, Anchit Gupta, Sonal Gupta, and Yashar Mehdad. 2021. MTOP: A comprehensive multilingual task-oriented semantic parsing benchmark. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 2950-2962, Online. Association for Computational Linguistics."
      },
      "source": "doi:10.48550/arXiv.2302.05454",
      "target": "doi:10.18653/v1/2021.eacl-main.257"
    },
    {
      "data": {
        "citation_text": "Ilya Loshchilov and Frank Hutter. 2017. Fixing weight decay regularization in adam. ArXiv, abs/1711.05101"
      },
      "source": "doi:10.48550/arXiv.2302.05454",
      "target": "abs/1711.05101"
    },
    {
      "data": {
        "citation_text": "Subhabrata Mukherjee and Ahmed Hassan Awadallah. 2020. XtremeDistil: Multi-stage distillation for massive multilingual models. In Proceedings of the 58th Annual Meeting of the Association for Computational"
      },
      "source": "doi:10.48550/arXiv.2302.05454",
      "target": "doi:10.18653/v1/2020.acl-main.202"
    },
    {
      "data": {
        "citation_text": "Linguistics, pages 2221-2234, Online. Association for Computational Linguistics."
      },
      "source": "doi:10.48550/arXiv.2302.05454",
      "target": "title:linguistics--pages-2221-2234--online--association-for-computational-linguistics-"
    },
    {
      "data": {
        "citation_text": "Giovanni Paolini, Ben Athiwaratkun, Jason Krone, Jie Ma, Alessandro Achille, Rishita Anubhai, Cicero Nogueira dos Santos, Bing Xiang, and Stefano Soatto. 2021. Structured prediction as translation between augmented natural languages."
      },
      "source": "doi:10.48550/arXiv.2302.05454",
      "target": "title:structured-prediction-as-translation-between-augmented-natural-languages"
    },
    {
      "data": {
        "citation_text": "StanfordNLP. GloVe: Global Vectors for Word Representation"
      },
      "source": "doi:10.48550/arXiv.2302.05454",
      "target": "doi:10.3115/v1/D14-1162"
    },
    {
      "data": {
        "citation_text": "Chengwei Qin and Shafiq Joty. 2021. Lfpt5: A unified framework for lifelong few-shot language learning based on prompt tuning of t 5 ."
      },
      "source": "doi:10.48550/arXiv.2302.05454",
      "target": "title:lfpt5--a-unified-framework-for-lifelong-few-shot-language-learning-based-on-prompt-tuning-of-t-5"
    },
    {
      "data": {
        "citation_text": "Alec Radford, Karthik Narasimhan, and Tim Salimansand Ilya Sutskever. 2018. Improving language understanding by generative pre-training. https: //www.cs.ubc.ca/ amuham01/LING530/ papers/radford2018improving.pdf"
      },
      "source": "doi:10.48550/arXiv.2302.05454",
      "target": "title:improving-language-understanding-by-generative-pre-training"
    },
    {
      "data": {
        "citation_text": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Stuskever. 2019. Language models are unsupervised multitask learners. Technical report, OpenAI."
      },
      "source": "doi:10.48550/arXiv.2302.05454",
      "target": "title:language-models-are-unsupervised-multitask-learners"
    },
    {
      "data": {
        "citation_text": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1-67."
      },
      "source": "doi:10.48550/arXiv.2302.05454",
      "target": "title:exploring-the-limits-of-transfer-learning-with-a-unified-text-to-text-transformer"
    },
    {
      "data": {
        "citation_text": "Karthik Raman, Iftekhar Naim, Jiecao Chen, Kazuma Hashimoto, Kiran Yalasangi, and Krishna Srinivasan. 2022. Transforming sequence tagging into a seq2seq task."
      },
      "source": "doi:10.48550/arXiv.2302.05454",
      "target": "doi:10.48550/arXiv.2203.08378"
    },
    {
      "data": {
        "citation_text": "Sebastian Schuster, Sonal Gupta, Rushin Shah, and Mike Lewis. 2019. Cross-lingual transfer learning for multilingual task oriented dialog. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3795-3805, Minneapolis, Minnesota. Association for Computational Linguistics."
      },
      "source": "doi:10.48550/arXiv.2302.05454",
      "target": "doi:10.18653/v1/N19-1380"
    },
    {
      "data": {
        "citation_text": "Sam Shleifer and Alexander M. Rush. 2020. Pre-trained summarization distillation."
      },
      "source": "doi:10.48550/arXiv.2302.05454",
      "target": "title:pre-trained-summarization-distillation"
    },
    {
      "data": {
        "citation_text": "Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova, and Jimmy Lin. 2019. Distilling taskspecific knowledge from bert into simple neural networks."
      },
      "source": "doi:10.48550/arXiv.2302.05454",
      "target": "title:distilling-taskspecific-knowledge-from-bert-into-simple-neural-networks"
    },
    {
      "data": {
        "citation_text": "Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition. In"
      },
      "source": "doi:10.48550/arXiv.2302.05454",
      "target": "doi:10.3115/1119176.1119195"
    },
    {
      "data": {
        "citation_text": "Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003, pages 142147"
      },
      "source": "doi:10.48550/arXiv.2302.05454",
      "target": "title:proceedings-of-the-seventh-conference-on-natural-language-learning-at-hlt-naacl-2003--pages-142147"
    },
    {
      "data": {
        "citation_text": "Iulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Well-read students learn better: On the importance of pre-training compact models."
      },
      "source": "doi:10.48550/arXiv.2302.05454",
      "target": "title:well-read-students-learn-better--on-the-importance-of-pre-training-compact-models"
    },
    {
      "data": {
        "citation_text": "Linting Xue, Aditya Barua, Noah Constant, Rami AlRfou, Sharan Narang, Mihir Kale, Adam Roberts, and Colin Raffel. 2022. Byt5: Towards a token-free future with pre-trained byte-to-byte models. Transactions of the Association for Computational Linguistics, 10:291-306."
      },
      "source": "doi:10.48550/arXiv.2302.05454",
      "target": "doi:10.1162/tacl_a_00461"
    },
    {
      "data": {
        "citation_text": "Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021. mT5: A massively multilingual pre-trained text-to-text transformer. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 483-498, Online. Association for Computational Linguistics."
      },
      "source": "doi:10.48550/arXiv.2302.05454",
      "target": "doi:10.18653/V1/2021.NAACL-MAIN.41"
    },
    {
      "data": {
        "citation_text": "Hang Yan, Tao Gui, Junqi Dai, Qipeng Guo, Zheng Zhang, and Xipeng Qiu. 2021. A unified generative framework for various NER subtasks. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 5808-5822, Online. Association for Computational Linguistics."
      },
      "source": "doi:10.48550/arXiv.2302.05454",
      "target": "doi:10.18653/v1/2021.acl-long.451"
    },
    {
      "data": {
        "citation_text": "Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. Opt: Open pretrained transformer language models."
      },
      "source": "doi:10.48550/arXiv.2302.05454",
      "target": "doi:10.48550/arXiv.2301.00774"
    },
    {
      "data": {
        "citation_text": "Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, and Chris Dyer. 2016. Neural architectures for named entity recognition."
      },
      "source": "doi:10.48550/arXiv.2302.05454",
      "target": "doi:10.18653/v1/N16-1030"
    },
    {
      "data": {
        "citation_text": "Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova, and Jimmy Lin. 2019. Distilling task-specific knowledge from bert into simple neural networks."
      },
      "source": "doi:10.48550/arXiv.2302.05454",
      "target": "title:distilling-task-specific-knowledge-from-bert-into-simple-neural-networks"
    },
    {
      "data": {
        "citation_text": null
      },
      "source": "doi:10.48550/arXiv.2309.07990",
      "target": "doi:10.48550/arXiv.2309.07990"
    },
    {
      "data": {
        "citation_text": "Alan Akbik, Tanja Bergmann, Duncan Blythe, Kashif Rasul, Stefan Schweter, and Roland Vollgraf. 2019. FLAIR: An easy-to-use framework for state-of-the-art NLP. In NAACL 2019, 2019 Annual Conference of the North American Chapter of the Association for"
      },
      "source": "doi:10.48550/arXiv.2309.07990",
      "target": "doi:10.18653/v1/N19-4010"
    },
    {
      "data": {
        "citation_text": "Computational Linguistics (Demonstrations), pages 54-59."
      },
      "source": "doi:10.48550/arXiv.2309.07990",
      "target": "title:computational-linguistics--demonstrations---pages-54-59-"
    },
    {
      "data": {
        "citation_text": "Zakariae Alami Merrouni, Bouchra Frikh, and Brahim Oubbi. 2020. Automatic keyphrase extraction: a survey and trends. Journal of Intelligent Information Systems, 54(2):391-424."
      },
      "source": "doi:10.48550/arXiv.2309.07990",
      "target": "doi:10.1007/s10844-019-00558-9"
    },
    {
      "data": {
        "citation_text": "Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The long-document transformer."
      },
      "source": "doi:10.48550/arXiv.2309.07990",
      "target": "title:longformer--the-long-document-transformer"
    },
    {
      "data": {
        "citation_text": "David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. J. Mach. Learn. Res., 3(null):993-1022"
      },
      "source": "doi:10.48550/arXiv.2309.07990",
      "target": "title:latent-dirichlet-allocation"
    },
    {
      "data": {
        "citation_text": "Tianqi Chen, Tong He, Michael Benesty, Vadim Khotilovich, Yuan Tang, Hyunsu Cho, Kailong Chen, et al. 2015. XGBoost: Extreme Gradient Boosting. $R$ package version 0.4-2, 1(4):1-4"
      },
      "source": "doi:10.48550/arXiv.2309.07990",
      "target": "doi:10.37859/coscitech.v3i3.4356"
    },
    {
      "data": {
        "citation_text": "Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416"
      },
      "source": "doi:10.48550/arXiv.2309.07990",
      "target": "doi:10.48550/arXiv.2210.11416"
    },
    {
      "data": {
        "citation_text": "Leon Derczynski, Kalina Bontcheva, and Ian Roberts. 2016. Broad Twitter corpus: A diverse named entity recognition resource. In Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 11691179, Osaka, Japan. The COLING 2016 Organizing Committee."
      },
      "source": "doi:10.48550/arXiv.2309.07990",
      "target": "title:broad-twitter-corpus--a-diverse-named-entity-recognition-resource"
    },
    {
      "data": {
        "citation_text": "Milan Dojchinovski, Dinesh Reddy, Tom\u00e1\u0161 Kliegr, Tom\u00e1\u0161 Vitvar, and Harald Sack. 2016. Crowdsourced corpus with entity salience annotations. In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16), pages 3307-3311, Portoro\u017e, Slovenia. European Language Resources Association (ELRA)."
      },
      "source": "doi:10.48550/arXiv.2309.07990",
      "target": "title:crowdsourced-corpus-with-entity-salience-annotations"
    },
    {
      "data": {
        "citation_text": "Jesse Dunietz and Daniel Gillick. 2014. A new entity salience task with millions of training examples. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, volume 2: Short Papers, pages 205-209, Gothenburg, Sweden. Association for Computational Linguistics."
      },
      "source": "doi:10.48550/arXiv.2309.07990",
      "target": "doi:10.3115/v1/E14-4040"
    },
    {
      "data": {
        "citation_text": "G\u00fcnes Erkan and Dragomir R. Radev. 2004. Lexrank: Graph-based lexical centrality as salience in text summarization. J. Artif. Int. Res., 22(1):457-479"
      },
      "source": "doi:10.48550/arXiv.2309.07990",
      "target": "doi:10.1613/jair.1523"
    },
    {
      "data": {
        "citation_text": "Michael Gamon, Tae Yano, Xinying Song, Johnson Apacible, and Patrick Pantel. 2013. Understanding document aboutness step one: Identifying salient entities. Technical Report MSR-TR-2013-73"
      },
      "source": "doi:10.48550/arXiv.2309.07990",
      "target": "title:understanding-document-aboutness-step-one--identifying-salient-entities"
    },
    {
      "data": {
        "citation_text": "Pengcheng He, Jianfeng Gao, and Weizhu Chen. 2023. DeBERTav3: Improving deBERTa using ELECTRAstyle pre-training with gradient-disentangled embedding sharing. In The Eleventh International Conference on Learning Representations."
      },
      "source": "doi:10.48550/arXiv.2309.07990",
      "target": "title:debertav3--improving-deberta-using-electrastyle-pre-training-with-gradient-disentangled-embedding-sharing"
    },
    {
      "data": {
        "citation_text": "Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bordino, Hagen F\u00fcrstenau, Manfred Pinkal, Marc Spaniol, Bilyana Taneva, Stefan Thater, and Gerhard Weikum. 2011. Robust disambiguation of named entities in text. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 782-792, Edinburgh, Scotland, UK. Association for Computational Linguistics."
      },
      "source": "doi:10.48550/arXiv.2309.07990",
      "target": "title:robust-disambiguation-of-named-entities-in-text"
    },
    {
      "data": {
        "citation_text": "Ella Hofmann-Coyle, Mayank Kulkarni, Lingjue Xie, Mounica Maddela, and Daniel Preotiuc-Pietro. 2022. Extractive entity-centric summarization as sentence selection using bi-encoders. In Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 326-333, Online only. Association for Computational Linguistics."
      },
      "source": "doi:10.48550/arXiv.2309.07990",
      "target": "doi:10.18653/v1/2022.aacl-short.40"
    },
    {
      "data": {
        "citation_text": "Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. 2006. OntoNotes: The $90 % solution. In Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers, pages 57-60, New York City, USA. Association for Computational Linguistics."
      },
      "source": "doi:10.48550/arXiv.2309.07990",
      "target": "doi:10.3115/1614049.1614064"
    },
    {
      "data": {
        "citation_text": "Xiaolei Huang and Michael J. Paul. 2018. Examining temporality in document classification. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 694-699, Melbourne, Australia. Association for Computational Linguistics."
      },
      "source": "doi:10.48550/arXiv.2309.07990",
      "target": "doi:10.18653/v1/P18-2110"
    },
    {
      "data": {
        "citation_text": "Anette Hulth. 2003. Improved automatic keyword extraction given more linguistic knowledge. In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing, pages 216-223."
      },
      "source": "doi:10.48550/arXiv.2309.07990",
      "target": "doi:10.3115/1119355.1119383"
    },
    {
      "data": {
        "citation_text": "Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu. 2017. Lightgbm: A highly efficient gradient boosting decision tree. Advances in neural information processing systems, 30."
      },
      "source": "doi:10.48550/arXiv.2309.07990",
      "target": "title:lightgbm--a-highly-efficient-gradient-boosting-decision-tree"
    },
    {
      "data": {
        "citation_text": "Nikolaos Kolitsas, Octavian-Eugen Ganea, and Thomas Hofmann. 2018. End-to-end neural entity linking. In Proceedings of the 22nd Conference on Computational Natural Language Learning, pages 519-529, Brussels, Belgium. Association for Computational Linguistics."
      },
      "source": "doi:10.48550/arXiv.2309.07990",
      "target": "doi:10.18653/v1/K18-1050"
    },
    {
      "data": {
        "citation_text": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A robustly optimized BERT pretraining approach. arXiv preprint arXiv:1907.11692."
      },
      "source": "doi:10.48550/arXiv.2309.07990",
      "target": "arXiv:1907.11692"
    },
    {
      "data": {
        "citation_text": "Ilya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization. In International Conference on Learning Representations."
      },
      "source": "doi:10.48550/arXiv.2309.07990",
      "target": "title:decoupled-weight-decay-regularization"
    },
    {
      "data": {
        "citation_text": "Mounica Maddela, Mayank Kulkarni, and Daniel Preotiuc-Pietro. 2022. EntSUM: A data set for entitycentric extractive summarization. In Proceedings of"
      },
      "source": "doi:10.48550/arXiv.2309.07990",
      "target": "title:entsum--a-data-set-for-entitycentric-extractive-summarization"
    },
    {
      "data": {
        "citation_text": "the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3355-3366, Dublin, Ireland. Association for Computational Linguistics"
      },
      "source": "doi:10.48550/arXiv.2309.07990",
      "target": "title:the-60th-annual-meeting-of-the-association-for-computational-linguistics--volume-1--long-papers---pages-3355-3366--dublin--ireland--association-for-computational-linguistics"
    },
    {
      "data": {
        "citation_text": "Edgar Meij, Wouter Weerkamp, and Maarten de Rijke. 2012. Adding semantics to microblog posts. In Proceedings of the Fifth ACM International Conference on Web Search and Data Mining, WSDM '12, page 563-572, New York, NY, USA. Association for Computing Machinery."
      },
      "source": "doi:10.48550/arXiv.2309.07990",
      "target": "doi:10.1145/2124295.2124364"
    },
    {
      "data": {
        "citation_text": "Rada Mihalcea and Paul Tarau. 2004. TextRank: Bringing order into text. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 404-411, Barcelona, Spain. Association for Computational Linguistics."
      },
      "source": "doi:10.48550/arXiv.2309.07990",
      "target": "title:textrank--bringing-order-into-text"
    },
    {
      "data": {
        "citation_text": "Dat Ba Nguyen, Johannes Hoffart, Martin Theobald, and Gerhard Weikum. 2014. Aida-light: High-throughput named-entity disambiguation. In Proceedings of the Workshop on Linked Data on the Web co-located with the 23rd International World Wide Web Conference (WWW 2014), Seoul, Korea, April 8, 2014, volume 1184 of CEUR Workshop Proceedings. CEURWS.org.\nF. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:28252830."
      },
      "source": "doi:10.48550/arXiv.2309.07990",
      "target": "title:aida-light--high-throughput-named-entity-disambiguation"
    },
    {
      "data": {
        "citation_text": "Marco Ponza, Diego Ceccarelli, Paolo Ferragina, Edgar Meij, and Sambhav Kothari. 2021. Contextualizing trending entities in news stories. In Proceedings of the 14th ACM International Conference on Web Search and Data Mining, pages 346-354."
      },
      "source": "doi:10.48550/arXiv.2309.07990",
      "target": "doi:10.1145/3437963.3441765"
    },
    {
      "data": {
        "citation_text": "Marco Ponza, Paolo Ferragina, and Francesco Piccinno. 2019. Swat: A system for detecting salient wikipedia entities in texts. Computational Intelligence, 35(4):858-890."
      },
      "source": "doi:10.48550/arXiv.2309.07990",
      "target": "doi:10.1111/coin.12216"
    },
    {
      "data": {
        "citation_text": "Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, Hwee Tou Ng, Anders Bj\u00f6rkelund, Olga Uryupina, Yuchen Zhang, and Zhi Zhong. 2013. Towards robust linguistic analysis using OntoNotes. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 143-152, Sofia, Bulgaria. Association for Computational Linguistics."
      },
      "source": "doi:10.48550/arXiv.2309.07990",
      "target": "title:towards-robust-linguistic-analysis-using-ontonotes"
    },
    {
      "data": {
        "citation_text": "Shruti Rijhwani and Daniel Preotiuc-Pietro. 2020. Temporally-informed analysis of named entity recognition. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7605-7617, Online. Association for Computational Linguistics."
      },
      "source": "doi:10.48550/arXiv.2309.07990",
      "target": "doi:10.18653/v1/2020.acl-main.680"
    },
    {
      "data": {
        "citation_text": "Evan Sandhaus. 2008. The new york times annotated corpus. Linguistic Data Consortium, Philadelphia, 6(12):e26752"
      },
      "source": "doi:10.48550/arXiv.2309.07990",
      "target": "doi:10.1007/978-3-642-41360-5_7"
    },
    {
      "data": {
        "citation_text": "Benjamin Strauss, Bethany Toma, Alan Ritter, MarieCatherine De Marneffe, and Wei Xu. 2016. Results of the w-nut 2016 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), pages 138-144."
      },
      "source": "doi:10.48550/arXiv.2309.07990",
      "target": "title:results-of-the-w-nut-2016-named-entity-recognition-shared-task"
    },
    {
      "data": {
        "citation_text": "Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung, Siamak Shakeri, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Denny Zhou, Neil Houlsby, and Donald Metzler. 2023. UI2: Unifying language learning paradigms."
      },
      "source": "doi:10.48550/arXiv.2309.07990",
      "target": "title:ui2--unifying-language-learning-paradigms"
    },
    {
      "data": {
        "citation_text": "Ian Tenney, Dipanjan Das, and Ellie Pavlick. 2019. BERT rediscovers the classical NLP pipeline. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 45934601, Florence, Italy. Association for Computational Linguistics."
      },
      "source": "doi:10.48550/arXiv.2309.07990",
      "target": "doi:10.18653/v1/P19-1452"
    },
    {
      "data": {
        "citation_text": "Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R Thomas McCoy, Najoung Kim, Benjamin Van Durme, Samuel R Bowman, Dipanjan Das, et al. 2018. What do you learn from context? probing for sentence structure in contextualized word representations. In International Conference on Learning Representations."
      },
      "source": "doi:10.48550/arXiv.2309.07990",
      "target": "title:what-do-you-learn-from-context--probing-for-sentence-structure-in-contextualized-word-representations"
    },
    {
      "data": {
        "citation_text": "Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the CoNLL-2003 shared task: Languageindependent named entity recognition. In Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003, pages 142-147."
      },
      "source": "doi:10.48550/arXiv.2309.07990",
      "target": "doi:10.3115/1119176.1119195"
    },
    {
      "data": {
        "citation_text": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open foundation and finetuned chat models."
      },
      "source": "doi:10.48550/arXiv.2309.07990",
      "target": "title:llama-2--open-foundation-and-finetuned-chat-models"
    },
    {
      "data": {
        "citation_text": "Salvatore Trani, Claudio Lucchese, Raffaele Perego, David E. Losada, Diego Ceccarelli, and Salvatore Orlando. 2018. Sel: A unified algorithm for salient entity linking. Computational Intelligence, 34(1):229."
      },
      "source": "doi:10.48550/arXiv.2309.07990",
      "target": "doi:10.1111/coin.12147"
    },
    {
      "data": {
        "citation_text": "Chuan Wu, Evangelos Kanoulas, Maarten de Rijke, and Wei Lu. 2020a. Wn-salience: A corpus of news articles with entity salience annotations. In Proceedings of The 12th Language Resources and Evaluation Conference, pages 2095-2102."
      },
      "source": "doi:10.48550/arXiv.2309.07990",
      "target": "title:wn-salience--a-corpus-of-news-articles-with-entity-salience-annotations"
    },
    {
      "data": {
        "citation_text": "Chuan Wu, Evangelos Kanoulas, and Maarten de Rijke. 2020b. Learning entity-centric document representations using an entity facet topic model. Inf. Process. Manage., 57(3)."
      },
      "source": "doi:10.48550/arXiv.2309.07990",
      "target": "doi:10.1016/j.ipm.2020.102216"
    },
    {
      "data": {
        "citation_text": "Chenyan Xiong, Zhengzhong Liu, Jamie Callan, and Tie-Yan Liu. 2018. Towards better text understanding and retrieval through kernel entity salience modeling. In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, pages 575-584."
      },
      "source": "doi:10.48550/arXiv.2309.07990",
      "target": "doi:10.1145/3209978.3209982"
    },
    {
      "data": {
        "citation_text": "Lingyun Zhao, Lin Li, Xinhao Zheng, and Jianwei Zhang. 2021. A bert based sentiment analysis and key entity detection approach for online financial texts. In 2021 IEEE 24th International Conference on Computer Supported Cooperative Work in Design (CSCWD), pages 1233-1238. IEEE."
      },
      "source": "doi:10.48550/arXiv.2309.07990",
      "target": "doi:10.1109/CSCWD49262.2021.9437616"
    },
    {
      "data": {
        "citation_text": "# Distillation of encoder-decoder transformers for sequence labellinging \n\nMarco Farina ${ }^{a 1}$ Duccio Pappadopulo ${ }^{a 1}$ Anant Gupta ${ }^{1}$ Leslie Huang ${ }^{1}$ Ozan \u0130rsoy ${ }^{1}$ Thamar Solorio ${ }^{11,2}$ { }^{1} Bloomberg { }^{2} Department of Computer Science, University of Houston {mfarina19, dpappadopulo, aqupta968, lhuang328, oirsoy}@bloomberg.net tsolorio@uh.edu \n\n#### Abstract\n\nDriven by encouraging results on a wide range of tasks, the field of NLP is experiencing an accelerated race to develop bigger language models. This race for bigger models has also underscored the need to continue the pursuit of practical distillation approaches that can leverage the knowledge acquired by these big models in a compute-efficient manner. Having this goal in mind, we build on recent work to propose a hallucination-free framework for sequence tagging that is especially suited for distillation. We show empirical results of new state-of-the-art performa"
      },
      "source": "title:distillation-of-encoder-decoder-transformers-for-sequence-labellinging",
      "target": "title:distillation-of-encoder-decoder-transformers-for-sequence-labellinging"
    }
  ]
}
